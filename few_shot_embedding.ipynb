{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../PythonAPI')\n",
    "sys.path.append('../PythonAPI/pycocotools')\n",
    "sys.path.append('./')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from cbas import CBAS\n",
    "from pycocotools.coco import COCO\n",
    "# from pycocotools.cbas import CBAS\n",
    "import cbas_construction_utils as ccu\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "idToName:  80 {'1': 'person', '2': 'bicycle', '3': 'car', '4': 'motorcycle', '5': 'airplane', '6': 'bus', '7': 'train', '8': 'truck', '9': 'boat', '10': 'traffic light', '11': 'fire hydrant', '13': 'stop sign', '14': 'parking meter', '15': 'bench', '16': 'bird', '17': 'cat', '18': 'dog', '19': 'horse', '20': 'sheep', '21': 'cow', '22': 'elephant', '23': 'bear', '24': 'zebra', '25': 'giraffe', '27': 'backpack', '28': 'umbrella', '31': 'handbag', '32': 'tie', '33': 'suitcase', '34': 'frisbee', '35': 'skis', '36': 'snowboard', '37': 'sports ball', '38': 'kite', '39': 'baseball bat', '40': 'baseball glove', '41': 'skateboard', '42': 'surfboard', '43': 'tennis racket', '44': 'bottle', '46': 'wine glass', '47': 'cup', '48': 'fork', '49': 'knife', '50': 'spoon', '51': 'bowl', '52': 'banana', '53': 'apple', '54': 'sandwich', '55': 'orange', '56': 'broccoli', '57': 'carrot', '58': 'hot dog', '59': 'pizza', '60': 'donut', '61': 'cake', '62': 'chair', '63': 'couch', '64': 'potted plant', '65': 'bed', '67': 'dining table', '70': 'toilet', '72': 'tv', '73': 'laptop', '74': 'mouse', '75': 'remote', '76': 'keyboard', '77': 'cell phone', '78': 'microwave', '79': 'oven', '80': 'toaster', '81': 'sink', '82': 'refrigerator', '84': 'book', '85': 'clock', '86': 'vase', '87': 'scissors', '88': 'teddy bear', '89': 'hair drier', '90': 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# initialize COCO api for instance annotations and category info\n",
    "cbas80=CBAS('../annotations/{}.json'.format('cbas80'))\n",
    "\n",
    "# Get category index so we can go from image ids to category names\n",
    "idToName={}\n",
    "for c in cbas80.dataset['categories']:\n",
    "    idToName[str(c['id'])]=c['name']\n",
    "\n",
    "print(\"idToName: \", len(idToName.items()), idToName)\n",
    "# base set is cbas34, and holdout set is cbas80-cbas34\n",
    "# holdout is cbas80 - cbas34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cbas_34 base and holdout (AKA \"target\" classes) sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "base_train = torchvision.datasets.ImageFolder(root='../images/cbas34_train', transform=transform)\n",
    "base_train_loader = torch.utils.data.DataLoader(base_train, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "base_valid = torchvision.datasets.ImageFolder(root='../images/cbas34_val', transform=transform)\n",
    "base_valid_loader = torch.utils.data.DataLoader(base_valid, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# get index for curriculum sampling\n",
    "id2idx = {}\n",
    "for i,img in enumerate(base_train.imgs):\n",
    "    img_id_str = img[0].split('/')[4].split('.')[0]\n",
    "    id2idx[img_id_str] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['root', 'imgs', 'classes', 'class_to_idx', 'transform', 'target_transform', 'loader'])\n",
      "('../images/cbas34_train/airplane/1363089.jpg', 0)\n",
      "Base classes (34 total): \n",
      "['airplane', 'backpack', 'banana', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'car', 'carrot', 'chair', 'clock', 'cow', 'cup', 'donut', 'fork', 'handbag', 'horse', 'kite', 'knife', 'person', 'pottedplant', 'sheep', 'sink', 'skateboard', 'spoon', 'surfboard', 'tennisracket', 'trafficlight', 'truck', 'umbrella', 'vase']\n",
      "base_train size:  51000\n",
      "steps per epoch:  12750\n"
     ]
    }
   ],
   "source": [
    "print(base_train.__dict__.keys())\n",
    "print(base_train.imgs[3])\n",
    "print(\"Base classes ({} total): \".format(len(base_train.classes)))\n",
    "print(base_train.classes)\n",
    "\n",
    "print(\"base_train size: \", base_train.__len__())\n",
    "print(\"steps per epoch: \", int(base_train.__len__() / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the holdout categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = COCO.loadCats(COCO.getCatIds())\n",
    "# nms=[cat['name'] for cat in all_categories]\n",
    "# print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "# Get all categories: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network that learns to predict images from base_set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this now that we have pytorch-classifcation/: \n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# __all__ = ['alexnet3232']\n",
    "\n",
    "\n",
    "# class AlexNet3232(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_classes=34):\n",
    "#         super(AlexNet3232, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def alexnet3232(**kwargs):\n",
    "#     r\"\"\"AlexNet model architecture from the\n",
    "#     `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "#     \"\"\"\n",
    "#     model = AlexNet(**kwargs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# num_classes_base = 34\n",
    "# num_classes_holdout = 34\n",
    "\n",
    "# Load model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 34)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = LeNet()\n",
    "# model = AlexNet3232()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this cell now that we can train using pytorch-classifcation/cbas.py: \n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# epochs = 20\n",
    "# print_every = 100\n",
    "# steps_per_epoch = int(base_train.__len__() / 4)\n",
    "\n",
    "# for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(base_train_loader, 0):\n",
    "#         # get the inputs\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # wrap them in Variable\n",
    "#         inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.data[0]\n",
    "#         if i % print_every == (print_every-1):    # print every 2000 mini-batches\n",
    "#             print('epoch[%d/%d, %5d/%d], loss: %.3f' %\n",
    "#                   (epoch + 1, epochs, i + 1, steps_per_epoch, running_loss / print_every))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this cell now that we can train using pytorch-classifcation/cbas.py: \n",
    "# import os \n",
    "\n",
    "# # Save model trained on cbas-LS base set:\n",
    "# weights_dir = './weights/'\n",
    "# if not os.path.exists(weights_dir):\n",
    "#     os.makedirs(weights_dir)\n",
    "# # torch.save(model, os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./pytorch-classification\")\n",
    "sys.path.append(\"./pytorch-classification/models\")\n",
    "sys.path.append(\"./pytorch-classification/models/cifar/\")\n",
    "import os\n",
    "import cbas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import models.cifar as models\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_model(path, model=None): \n",
    "    gpu_id = \"0\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"use_cuda: \", use_cuda)\n",
    "    \n",
    "    if model is None: \n",
    "        model = models.alexnet(num_classes=34)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using GPU\")\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        model = model.cuda()\n",
    "        cudnn.benchmark = True\n",
    "        checkpoint = torch.load(path)\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else: \n",
    "        print(\"Using CPU (no GPU)\")\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        trained_on_gpu = list(state_dict.keys())[0].startswith('module.')\n",
    "        if trained_on_gpu:\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k,v in state_dict.items():\n",
    "                name = k[7:] # remove module.\n",
    "                new_state_dict[name] = v\n",
    "            state_dict = new_state_dict\n",
    "        else: \n",
    "            checkpoint = torch.load(path)\n",
    "            state_dict = checkpoint['state_dict']\n",
    "                \n",
    "    \n",
    "#     print(checkpoint.keys())\n",
    "    print(\"state_dict: \", checkpoint['state_dict'].keys())\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model, use_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda:  False\n",
      "Using CPU (no GPU)\n",
      "state_dict:  odict_keys(['module.features.0.weight', 'module.features.0.bias', 'module.features.3.weight', 'module.features.3.bias', 'module.features.6.weight', 'module.features.6.bias', 'module.features.8.weight', 'module.features.8.bias', 'module.features.10.weight', 'module.features.10.bias', 'module.classifier.weight', 'module.classifier.bias'])\n",
      "False AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(5, 5))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Linear(in_features=256, out_features=34)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Grab weights from my google drive: https://drive.google.com/drive/u/0/folders/1BFYG5soBG6vBV7RC00NjadPYdCapWLgb\n",
    "# (use your berkeley google account)\n",
    "model, use_cuda = load_model('./pytorch-classification/alexnet_300epochs/model_best.pth.tar')\n",
    "print(use_cuda, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \n",
    "    def __init__(self, model=None, embed_layer=None, embed_size=256, transform=None):\n",
    "        self.embed_size = embed_size\n",
    "        self.transforms = transform\n",
    "        \n",
    "        if model is None: \n",
    "            self.model = models.alexnet(pretrained=True)\n",
    "            self.embed_layer = self.model.features\n",
    "        else: \n",
    "            if embed_layer is None: \n",
    "                raise ValueError(\"Need to specify embed_layer if you pass in a model to FeatureExtractor!\")\n",
    "            self.model = model\n",
    "            self.embed_layer = embed_layer\n",
    "        \n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            \n",
    "        # Set model to eval mode so any train-specific things like dropout, etc. don't run:\n",
    "        self.model.eval()\n",
    "    \n",
    "    def embed(self, img):\n",
    "        \"\"\"\n",
    "        project a PIL image into embedded feature space, and return that vector as an np array\n",
    "        \"\"\"\n",
    "        # Work arround issue of some of the train images not being RGB, (they won't go through the CNN): \n",
    "        if img.mode != \"RGB\":\n",
    "            print(\"Hack: converting image to RGB before embedding w/ CNN...\")\n",
    "            rgbimg = Image.new(\"RGB\", img.size)\n",
    "            rgbimg.paste(img)\n",
    "            img = rgbimg\n",
    "\n",
    "        a = self.transforms(img)\n",
    "        image = Variable(a)\n",
    "        image = image.unsqueeze(0)\n",
    "        if self.cuda: image.cuda()\n",
    "        #print(image.size())\n",
    "        \n",
    "        embedding = torch.zeros(self.embed_size)\n",
    "        def copy_embedding(m, i, o):\n",
    "            if len(o.size()) > 2:\n",
    "                o = o.view(o.size(0), -1)\n",
    "            embedding.copy_(o.data)\n",
    "            \n",
    "        h = self.embed_layer.register_forward_hook(copy_embedding)\n",
    "        h_x = self.model(image)\n",
    "        h.remove()\n",
    "        return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# # Load model pre-trained on cbas-LS base set: \n",
    "# weights_dir = './weights/'\n",
    "# if not os.path.exists(weights_dir):\n",
    "#     os.makedirs(weights_dir)\n",
    "\n",
    "# model = torch.load(os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    embedder = FeatureExtractor(model=model, transform=transform, embed_layer=model.module.features)\n",
    "else:\n",
    "    embedder = FeatureExtractor(model=model, transform=transform, embed_layer=model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,) [  2.60561407e-01   0.00000000e+00   1.05016306e-01   1.45645487e+00\n",
      "   6.41261041e-01   0.00000000e+00   0.00000000e+00   7.01496005e-01\n",
      "   6.56437576e-01   1.20135117e+00   3.96289885e-01   0.00000000e+00\n",
      "   7.33434111e-02   0.00000000e+00   3.52962971e-01   1.93643004e-01\n",
      "   1.22039044e+00   1.04992640e+00   6.91160321e-01   3.86048675e-01\n",
      "   0.00000000e+00   0.00000000e+00   1.83523893e-02   3.14123333e-02\n",
      "   6.47000000e-02   2.17227265e-01   8.42344835e-02   7.58793414e-01\n",
      "   4.54706043e-01   0.00000000e+00   9.94468108e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.26395068e-02\n",
      "   5.14534295e-01   3.08245689e-01   5.23454666e-01   7.32052147e-01\n",
      "   7.76769996e-01   6.68430030e-01   7.16127977e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.88239816e-02\n",
      "   0.00000000e+00   1.84420025e+00   4.97282177e-01   8.67667079e-01\n",
      "   0.00000000e+00   0.00000000e+00   9.91738796e-01   1.03220630e+00\n",
      "   6.44936204e-01   3.83864939e-02   1.36903310e+00   1.45415962e-01\n",
      "   0.00000000e+00   3.48791778e-02   0.00000000e+00   0.00000000e+00\n",
      "   4.99736339e-01   1.80694926e+00   3.92764151e-01   7.39266217e-01\n",
      "   0.00000000e+00   5.13008535e-01   0.00000000e+00   1.80655193e+00\n",
      "   3.30201715e-01   0.00000000e+00   6.14017487e-01   0.00000000e+00\n",
      "   1.58587798e-01   1.09168029e+00   3.51875514e-01   5.25640309e-01\n",
      "   8.78890693e-01   0.00000000e+00   0.00000000e+00   7.75238156e-01\n",
      "   1.07723832e+00   0.00000000e+00   1.29824400e-01   6.20662749e-01\n",
      "   1.46498829e-01   0.00000000e+00   0.00000000e+00   1.47892666e+00\n",
      "   3.44131500e-01   0.00000000e+00   1.37936437e+00   5.14341965e-02\n",
      "   5.02116799e-01   7.56435171e-02   0.00000000e+00   3.90397608e-01\n",
      "   9.67830062e-01   4.93403316e-01   2.45528984e+00   0.00000000e+00\n",
      "   8.39444578e-01   1.63981974e+00   1.14268315e+00   3.50249335e-02\n",
      "   1.87422812e+00   9.29888904e-01   5.66391274e-02   1.75374568e+00\n",
      "   0.00000000e+00   1.55893898e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.37709570e-01   8.25599581e-02   0.00000000e+00   2.51467466e-01\n",
      "   1.29742891e-01   8.90296921e-02   0.00000000e+00   1.70147642e-02\n",
      "   2.09535217e+00   1.95163631e+00   1.28807485e-01   2.54317194e-01\n",
      "   0.00000000e+00   1.49415448e-01   0.00000000e+00   1.65789771e+00\n",
      "   1.05401024e-01   1.46742165e+00   0.00000000e+00   1.36475280e-01\n",
      "   0.00000000e+00   2.41863549e-01   7.39162326e-01   0.00000000e+00\n",
      "   6.36650562e-01   0.00000000e+00   1.64487731e+00   3.61905575e-01\n",
      "   0.00000000e+00   1.15093923e+00   1.13068140e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.50735044e-01   0.00000000e+00\n",
      "   1.33547395e-01   0.00000000e+00   5.37841201e-01   1.53294766e+00\n",
      "   6.62143156e-02   4.10982072e-02   0.00000000e+00   9.00968671e-01\n",
      "   5.42169869e-01   6.98568940e-01   1.85056776e-01   4.60502982e-01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.15671748e-01\n",
      "   1.28051484e+00   5.53446472e-01   1.64637852e+00   3.74263674e-01\n",
      "   0.00000000e+00   0.00000000e+00   2.15280354e-01   7.86138996e-02\n",
      "   3.41238528e-01   2.75817305e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.36380100e+00   0.00000000e+00   1.42920569e-01   1.96994722e-01\n",
      "   0.00000000e+00   1.32360756e-01   1.32600620e-01   1.25046051e+00\n",
      "   0.00000000e+00   4.52775449e-01   6.08076096e-01   6.52160764e-01\n",
      "   0.00000000e+00   9.24109459e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.90622902e+00   0.00000000e+00\n",
      "   6.54434413e-02   3.77908438e-01   7.32880116e-01   6.17764890e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.20193315e-01\n",
      "   1.15272805e-01   4.46733475e-01   0.00000000e+00   1.90381020e-01\n",
      "   0.00000000e+00   0.00000000e+00   1.13532412e+00   1.67707697e-01\n",
      "   2.43426204e-01   9.58338439e-01   1.90804839e+00   7.42027402e-01\n",
      "   7.83838987e-01   1.74595094e+00   1.65996194e+00   1.74196649e+00\n",
      "   8.31970811e-01   1.09458995e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.66439527e-01   1.30759680e+00   0.00000000e+00   4.98171598e-02\n",
      "   1.00429261e+00   1.76445484e-01   1.71944225e+00   0.00000000e+00\n",
      "   1.73524752e-01   3.69339228e-01   1.36320163e-02   0.00000000e+00\n",
      "   1.42209005e+00   3.81123424e-01   0.00000000e+00   7.66885698e-01\n",
      "   1.35393775e+00   2.23101139e+00   0.00000000e+00   5.48122972e-02\n",
      "   0.00000000e+00   1.82219672e+00   0.00000000e+00   2.52359182e-01]\n",
      "(256,) [ 0.05288631  0.          0.98272818  0.05508019  0.04559932  0.57868415\n",
      "  0.231464    0.          0.          0.          0.0312447   0.\n",
      "  0.26158944  0.          0.0749774   0.34636042  0.33924907  0.07087953\n",
      "  0.10298952  0.17138784  0.          0.42598617  0.20665044  0.1424918\n",
      "  0.1276738   0.05386052  0.          0.14801146  0.51940626  0.58368117\n",
      "  0.00958149  0.          0.09637089  0.06655493  0.          0.45996734\n",
      "  0.60752165  0.04262242  0.09900378  0.16794391  0.99169946  0.\n",
      "  0.17044914  0.          0.65178996  0.          0.2114889   0.55076677\n",
      "  0.26816201  0.17075191  0.23098814  1.06194949  0.          0.          0.\n",
      "  0.97859013  0.49441442  0.24045983  0.45859092  0.          0.45546976\n",
      "  0.          0.          0.          0.2372347   0.          0.\n",
      "  0.29865476  0.15953507  0.15719965  0.          0.12224975  0.14900151\n",
      "  0.47580791  0.03746127  0.          0.03680462  0.78560489  0.          0.0153982\n",
      "  0.19122715  0.18700272  0.30370885  0.20375156  0.6072371   0.\n",
      "  0.26577824  0.22072627  0.          0.64705408  0.06816272  0.48045683\n",
      "  0.1613518   0.3468715   0.11909489  0.          0.33859429  0.146185    0.\n",
      "  0.          0.55409199  0.10721465  0.13607082  0.          0.          0.\n",
      "  0.          0.17972867  1.03378832  0.          0.18256931  0.42431492\n",
      "  0.11153528  0.          0.          0.67562604  0.27695942  0.10193281\n",
      "  0.          0.          0.49969941  0.35731336  0.54498231  0.0758613   0.\n",
      "  0.          0.37724429  0.34862277  0.          0.13489617  0.08706891\n",
      "  0.12213653  0.69429618  0.          0.05908936  0.34539133  0.26918933\n",
      "  0.          0.          0.          0.9265905   0.          0.2650218\n",
      "  0.77656132  0.          0.27137542  0.08633105  0.01202363  0.\n",
      "  0.03859537  0.45290238  0.05689093  0.          0.42668968  0.\n",
      "  0.21317863  0.32399088  0.          0.          0.          0.10379828\n",
      "  0.13201573  0.16614325  0.05623975  0.2672492   0.          0.          0.4540475\n",
      "  0.11667694  0.44282141  0.          0.          0.          0.19744784\n",
      "  0.          0.05745433  0.          0.          0.20770203  0.          0.\n",
      "  0.51428765  0.          0.06106886  0.19679287  0.32694051  0.02390924\n",
      "  0.05079201  0.54085058  0.          0.          0.08279427  0.62330991\n",
      "  0.47441131  0.          0.81496716  0.29089609  0.          0.14230934\n",
      "  0.54324222  0.          0.13159972  0.          0.88591295  0.7039265   0.\n",
      "  0.83263695  0.          0.0565638   0.04752846  1.42613292  0.39896512\n",
      "  0.45989457  0.52775532  0.84365785  0.33952817  0.88667786  0.99994451\n",
      "  0.          0.04493607  0.          0.41834608  0.40656725  0.\n",
      "  0.63866949  0.72184467  0.08086337  0.          0.          0.          0.\n",
      "  0.          0.41092193  0.48040813  0.32881334  0.02752321  0.\n",
      "  0.08844588  0.          0.          0.09226666  0.          0.47667447\n",
      "  0.          0.          0.          0.11191913  0.64991021  0.05871234\n",
      "  0.1769537   0.          0.          0.13677002  0.          0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get some random training images\n",
    "dataiter = iter(base_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Test embedding a couple of images. For Alexnet (32x32 images) this should be a 256-d vector: \n",
    "img = Image.open('../images/cbas34_train/airplane/156356.jpg')\n",
    "emb = embedder.embed(img)\n",
    "print(emb.shape, emb)\n",
    "\n",
    "img = Image.open('../images/cbas34_train/cow/69943.jpg')\n",
    "emb = embedder.embed(img)\n",
    "print(emb.shape, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Embedding Results\n",
    "Now let's compute cosine similarities for one airplane image against all airplanes, and then similarities for airplane against several other non-airplane categories. The average cosine similarity for airplane vs. airplane should be higher than that for airplane-vs-non-airplanes, if we are computing good embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  airplane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'airplane' intra-category similarity: 0.5330798383712768\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'cow'...\n",
      "average airplane cos-similarity vs 'cow': 0.43208702360391615\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'banana'...\n",
      "average airplane cos-similarity vs 'banana': 0.35064838972687723\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'sink'...\n",
      "average airplane cos-similarity vs 'sink': 0.39051078941822054\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'carrot'...\n",
      "Hack: converting ../images/cbas34_train/airplane/156497.jpg to rgb...\n",
      "average airplane cos-similarity vs 'carrot': 0.349950625705719\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'umbrella'...\n",
      "Hack: converting ../images/cbas34_train/airplane/247539.jpg to rgb...\n",
      "average airplane cos-similarity vs 'umbrella': 0.44700413351655005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.random.seed(55)\n",
    "\n",
    "def get_cos_similarities(base_img_path, other_image_paths):\n",
    "    img = Image.open(base_img_path)\n",
    "    # Work arround issue of some of the train images not being RGB, (they won't go through the CNN): \n",
    "    if img.mode != \"RGB\":\n",
    "        print(\"Hack: converting {} to rgb...\".format(base_img_path))\n",
    "        rgbimg = Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    base_embedding = embedder.embed(img)\n",
    "    similarities = np.zeros((len(other_image_paths)))\n",
    "    \n",
    "    for idx,img_path in enumerate(other_image_paths):\n",
    "        #print(\"idx: \", idx, img_path)\n",
    "        embedding = None\n",
    "        img = Image.open(img_path)\n",
    "        try:\n",
    "            embedding = embedder.embed(img)\n",
    "        except:\n",
    "            print(\"error on image #\", idx)\n",
    "        if embedding is not None: \n",
    "#             cos_sim = cosine_similarity(base_embedding, embedding)\n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            if use_cuda:\n",
    "                cos_sim = cos(torch.Tensor(base_embedding).cuda().unsqueeze(0), torch.Tensor(embedding).cuda().unsqueeze(0))\n",
    "            else: \n",
    "                cos_sim = cos(torch.Tensor(base_embedding).unsqueeze(0), torch.Tensor(embedding).unsqueeze(0))\n",
    "            similarities[idx] = cos_sim.cpu().numpy()\n",
    "    return similarities\n",
    "\n",
    "def avg_category_similarities(dataset_path, cat1, cat2, sample_size=50):\n",
    "    \"\"\"\n",
    "    Compute avereage similarities between all imgs in categories cat1 an cat2\n",
    "    \"\"\"\n",
    "    cat1_imgs = glob.glob(dataset_path.format(cat1)+'*.jpg')\n",
    "    cat2_imgs = glob.glob(dataset_path.format(cat2)+'*.jpg')\n",
    "    # Sample a limited size to reduce compute time: \n",
    "    size1, size2 = min(len(cat1_imgs), sample_size), min(len(cat2_imgs), sample_size)\n",
    "    cat1_imgs = np.random.choice(cat1_imgs, size=size1)\n",
    "    cat2_imgs = np.random.choice(cat2_imgs, size=size2)\n",
    "    sims = np.zeros((size1, size2))\n",
    "    #print(\"similarites shape: \", sims.shape)\n",
    "    for i,img1 in enumerate(cat1_imgs):\n",
    "        sims[i,:] = get_cos_similarities(img1, cat2_imgs).T\n",
    "        pass\n",
    "    return sims\n",
    "\n",
    "\n",
    "base_cat = \"airplane\"\n",
    "other_cats = [\"cow\", \"banana\", \"sink\", \"carrot\", \"umbrella\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbas34_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbas34_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))\n",
    "\n",
    "\n",
    "## NOTE: the error messages here are mostly due to some of the training \n",
    "## images in cbas-34 being 1 channel instead of 3. We should regenerate the \n",
    "## cbas set to be 3-channel to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  banana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'banana' intra-category similarity: 0.5884141979694366\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'cow'...\n",
      "average banana cos-similarity vs 'cow': 0.40151104258298875\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'airplane'...\n",
      "average banana cos-similarity vs 'airplane': 0.35424724360108373\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'sink'...\n",
      "average banana cos-similarity vs 'sink': 0.4279528742313385\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'carrot'...\n",
      "average banana cos-similarity vs 'carrot': 0.5344849278271199\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'umbrella'...\n",
      "average banana cos-similarity vs 'umbrella': 0.41140607810020446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"banana\"\n",
    "other_cats = [\"cow\", \"airplane\", \"sink\", \"carrot\", \"umbrella\"]\n",
    "\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbas34_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbas34_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we try embedding images from classes in the holdout set?\n",
    "\n",
    "In the preceding cells we did a sanity check to verify that embeddings made some kind of sense, but those were all embeddings the network was trained on. What about when we embed images for classes the network was not trained on? Let's try it out by doing similar comparisons for categories in the cbas-LS set (i.e., holdout set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  mouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'mouse' intra-category similarity: 0.5026432198226451\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'orange'...\n",
      "average mouse cos-similarity vs 'orange': 0.45833721430301666\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'suitcase'...\n",
      "average mouse cos-similarity vs 'suitcase': 0.45220311656594275\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'toilet'...\n",
      "average mouse cos-similarity vs 'toilet': 0.4934432402729988\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'elephant'...\n",
      "average mouse cos-similarity vs 'elephant': 0.38700988946557047\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'giraffe'...\n",
      "average mouse cos-similarity vs 'giraffe': 0.398318815779686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"mouse\"\n",
    "other_cats = [\"orange\", \"suitcase\", \"toilet\", \"elephant\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  orange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'orange' intra-category similarity: 0.6251163078546524\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'hotdog'...\n",
      "average orange cos-similarity vs 'hotdog': 0.5699484491109849\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'suitcase'...\n",
      "average orange cos-similarity vs 'suitcase': 0.37430530357956887\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'laptop'...\n",
      "average orange cos-similarity vs 'laptop': 0.38799188279509544\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'scissors'...\n",
      "average orange cos-similarity vs 'scissors': 0.4093845841884613\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'giraffe'...\n",
      "average orange cos-similarity vs 'giraffe': 0.37873646223545077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"orange\"\n",
    "other_cats = [\"hotdog\", \"suitcase\", \"laptop\", \"scissors\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'pizza' intra-category similarity: 0.5749508616626262\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'hotdog'...\n",
      "average pizza cos-similarity vs 'hotdog': 0.5535113266468048\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'remote'...\n",
      "average pizza cos-similarity vs 'remote': 0.4865093769550323\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'laptop'...\n",
      "average pizza cos-similarity vs 'laptop': 0.48646524358391763\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'stopsign'...\n",
      "average pizza cos-similarity vs 'stopsign': 0.4035489796221256\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'giraffe'...\n",
      "average pizza cos-similarity vs 'giraffe': 0.39467512319087983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"pizza\"\n",
    "other_cats = [\"hotdog\", \"remote\", \"laptop\", \"stopsign\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Try Few-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://xkcd.com/221/ \n",
    "np.random.seed(4)\n",
    "\n",
    "def get_joint_categories():\n",
    "    joint_cats = []\n",
    "    base_cats   = glob.glob('../images/cbas34_train/*/')\n",
    "    holdout_cats = glob.glob('../images/cbasLS_train/*/')\n",
    "    joint_cats.extend(base_cats)\n",
    "    joint_cats.extend(holdout_cats)\n",
    "    return joint_cats\n",
    "    \n",
    "# For each category in cbasLS, choose N examples from cbasLS train set:\n",
    "def get_holdout_train_examples(n):\n",
    "    \"\"\"\n",
    "    Gets a list of image paths from cbasLS train set. List contains n \n",
    "    randomly sampled images from each category in cbasLS\n",
    "    \"\"\"\n",
    "    ls_path = '../images/cbasLS_train/'\n",
    "    cats = glob.glob(ls_path + '*/')\n",
    "    train_example_paths = []\n",
    "    #print(len(cats), cats)\n",
    "    for cat_path in cats:\n",
    "        imgs = glob.glob(\"{}*.jpg\".format(cat_path))\n",
    "        train_example_paths.extend(np.random.choice(imgs, replace=False, size=n))\n",
    "    return train_example_paths \n",
    "\n",
    "def get_embeddings(img_paths):\n",
    "    \"\"\"\n",
    "    input: is list of image paths.\n",
    "    output: list of tuples of form: (category, id, path, vec. embedding)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (os.path.dirname(p).split('/')[-1]\n",
    "         , os.path.basename(p).replace('.jpg','')\n",
    "         , p\n",
    "         , embedder.embed(Image.open(p))\n",
    "        ) for p in img_paths\n",
    "    ]\n",
    "\n",
    "# Get embeddings for images in holdout validation set: \n",
    "def get_ls_valid_embeddings():\n",
    "    \"\"\"\n",
    "    output: list of tuples of form: (category, id, path, vec. embedding)\n",
    "    \"\"\"\n",
    "    valid_paths = glob.glob('../images/cbasLS_val/*/*.jpg')\n",
    "    print(\"Found {} validation images in cbasLS_val. Getting embeddings...\".format(len(valid_paths)))\n",
    "    embeddings = get_embeddings(valid_paths)\n",
    "    return embeddings\n",
    "\n",
    "def lowshot_valid_acc(X_valid, y_valid, y_pred):\n",
    "    \"\"\"\n",
    "    Returns validation accuracy \n",
    "    \"\"\"\n",
    "    if not ((len(X_valid) == len(y_valid)) and (len(X_valid)==len(y_pred))):\n",
    "        raise ValueError(\n",
    "            \"Lengths must be the same for: X_valid:{}, y_valid:{}, y_pred:{}\".format(\n",
    "            len(X_valid), len(y_valid), len(y_pred)\n",
    "        ))\n",
    "    total, total_acc = len(X_valid), 0\n",
    "    for i, x_val in enumerate(X_valid):\n",
    "        total_acc += 1 if y_valid[i]==y_pred[i] else 0\n",
    "    val_acc = float(total_acc) / float(total)\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed the images in the cbas low-shot valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lowshot predictor for N=1\n",
      "Found 8330 validation images in cbasLS_val. Getting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "/home/gbiamby/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "validation acc.: 0.09015606242496998\n",
      "Training lowshot predictor for N=2\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Found 8330 validation images in cbasLS_val. Getting embeddings...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "validation acc.: 0.08151260504201681\n",
      "Training lowshot predictor for N=5\n",
      "Hack: converting {} to rgb...\n",
      "Found 8330 validation images in cbasLS_val. Getting embeddings...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "validation acc.: 0.10204081632653061\n",
      "Training lowshot predictor for N=10\n",
      "Hack: converting {} to rgb...\n",
      "Found 8330 validation images in cbasLS_val. Getting embeddings...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "validation acc.: 0.13217286914765905\n",
      "Training lowshot predictor for N=20\n",
      "Hack: converting {} to rgb...\n",
      "Found 8330 validation images in cbasLS_val. Getting embeddings...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "Hack: converting {} to rgb...\n",
      "validation acc.: 0.16806722689075632\n"
     ]
    }
   ],
   "source": [
    "def lowshot_fit_and_predict(N, target_set=\"holdout\"):\n",
    "    \"\"\"\n",
    "    Trains knn classifier with N training examples from each category in cbasLS, \n",
    "    and returns validation accuracy on entire cbasLS_val set\n",
    "    inputs: \n",
    "        N: num. training examples per cbasLS_train category \n",
    "        target_set: (str) indicates which set of categories we train classifier with. \n",
    "                    Shuold either be: 'holdout', or 'joint'\n",
    "    \"\"\"\n",
    "    if target_set!=\"holdout\":\n",
    "        raise NotImplementedError(\"Only support for target_set='holdout' is available rn.\")\n",
    "    train_paths = get_holdout_train_examples(N)\n",
    "    train_samples = get_embeddings(train_paths)\n",
    "    # print(train_samples)\n",
    "    #joint_cats = get_joint_categories()\n",
    "    #print(\"Found {} total joint (base + holdout) cats\".format(len(joint_cats)))\n",
    "\n",
    "    # Fit a knn classifier to those N training examples: \n",
    "    X_train = [vec for cat,id,p,vec in train_samples]\n",
    "    y_train = [cat for cat,id,p,vec in train_samples]\n",
    "    knn = KNeighborsClassifier(n_neighbors=N)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    ls_valid_set = get_ls_valid_embeddings()\n",
    "    #print(ls_valid_set[0])\n",
    "    X_valid = [vec for cat,id,p,vec in ls_valid_set]\n",
    "    y_valid = [cat for cat,id,p,vec in ls_valid_set]\n",
    "    y_pred = knn.predict(X_valid)\n",
    "\n",
    "    # Val acc: \n",
    "    return lowshot_valid_acc(X_valid, y_valid, y_pred)\n",
    "\n",
    "N_vals = [1, 2, 5, 10, 20]\n",
    "\n",
    "for N in N_vals: \n",
    "    print(\"Training lowshot predictor for N={}\".format(N))\n",
    "    print(\"validation acc.: {}\".format(lowshot_fit_and_predict(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
