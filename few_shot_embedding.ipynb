{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../PythonAPI')\n",
    "sys.path.append('../PythonAPI/pycocotools')\n",
    "sys.path.append('./')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from cbas import CBAS\n",
    "from pycocotools.coco import COCO\n",
    "# from pycocotools.cbas import CBAS\n",
    "import cbas_construction_utils as ccu\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "idToName:  80 {'1': 'person', '2': 'bicycle', '3': 'car', '4': 'motorcycle', '5': 'airplane', '6': 'bus', '7': 'train', '8': 'truck', '9': 'boat', '10': 'traffic light', '11': 'fire hydrant', '13': 'stop sign', '14': 'parking meter', '15': 'bench', '16': 'bird', '17': 'cat', '18': 'dog', '19': 'horse', '20': 'sheep', '21': 'cow', '22': 'elephant', '23': 'bear', '24': 'zebra', '25': 'giraffe', '27': 'backpack', '28': 'umbrella', '31': 'handbag', '32': 'tie', '33': 'suitcase', '34': 'frisbee', '35': 'skis', '36': 'snowboard', '37': 'sports ball', '38': 'kite', '39': 'baseball bat', '40': 'baseball glove', '41': 'skateboard', '42': 'surfboard', '43': 'tennis racket', '44': 'bottle', '46': 'wine glass', '47': 'cup', '48': 'fork', '49': 'knife', '50': 'spoon', '51': 'bowl', '52': 'banana', '53': 'apple', '54': 'sandwich', '55': 'orange', '56': 'broccoli', '57': 'carrot', '58': 'hot dog', '59': 'pizza', '60': 'donut', '61': 'cake', '62': 'chair', '63': 'couch', '64': 'potted plant', '65': 'bed', '67': 'dining table', '70': 'toilet', '72': 'tv', '73': 'laptop', '74': 'mouse', '75': 'remote', '76': 'keyboard', '77': 'cell phone', '78': 'microwave', '79': 'oven', '80': 'toaster', '81': 'sink', '82': 'refrigerator', '84': 'book', '85': 'clock', '86': 'vase', '87': 'scissors', '88': 'teddy bear', '89': 'hair drier', '90': 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# initialize COCO api for instance annotations and category info\n",
    "cbas80=CBAS('../annotations/{}.json'.format('cbas80'))\n",
    "\n",
    "# Get category index so we can go from image ids to category names\n",
    "idToName={}\n",
    "for c in cbas80.dataset['categories']:\n",
    "    idToName[str(c['id'])]=c['name']\n",
    "\n",
    "print(\"idToName: \", len(idToName.items()), idToName)\n",
    "# base set is cbas34, and holdout set is cbas80-cbas34\n",
    "# holdout is cbas80 - cbas34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cbas_34 base and holdout (AKA \"target\" classes) sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "base_train = torchvision.datasets.ImageFolder(root='../images/cbas34_train', transform=transform)\n",
    "base_train_loader = torch.utils.data.DataLoader(base_train, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "base_valid = torchvision.datasets.ImageFolder(root='../images/cbas34_val', transform=transform)\n",
    "base_valid_loader = torch.utils.data.DataLoader(base_valid, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# get index for curriculum sampling\n",
    "id2idx = {}\n",
    "for i,img in enumerate(base_train.imgs):\n",
    "    img_id_str = img[0].split('/')[4].split('.')[0]\n",
    "    id2idx[img_id_str] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['root', 'imgs', 'classes', 'class_to_idx', 'transform', 'target_transform', 'loader'])\n",
      "('../images/cbas34_train/airplane/1363089.jpg', 0)\n",
      "Base classes (34 total): \n",
      "['airplane', 'backpack', 'banana', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'car', 'carrot', 'chair', 'clock', 'cow', 'cup', 'donut', 'fork', 'handbag', 'horse', 'kite', 'knife', 'person', 'pottedplant', 'sheep', 'sink', 'skateboard', 'spoon', 'surfboard', 'tennisracket', 'trafficlight', 'truck', 'umbrella', 'vase']\n",
      "base_train size:  51000\n",
      "steps per epoch:  12750\n"
     ]
    }
   ],
   "source": [
    "print(base_train.__dict__.keys())\n",
    "print(base_train.imgs[3])\n",
    "print(\"Base classes ({} total): \".format(len(base_train.classes)))\n",
    "print(base_train.classes)\n",
    "\n",
    "print(\"base_train size: \", base_train.__len__())\n",
    "print(\"steps per epoch: \", int(base_train.__len__() / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the holdout categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = COCO.loadCats(COCO.getCatIds())\n",
    "# nms=[cat['name'] for cat in all_categories]\n",
    "# print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "# Get all categories: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network that learns to predict images from base_set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this now that we have pytorch-classifcation/: \n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# __all__ = ['alexnet3232']\n",
    "\n",
    "\n",
    "# class AlexNet3232(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_classes=34):\n",
    "#         super(AlexNet3232, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def alexnet3232(**kwargs):\n",
    "#     r\"\"\"AlexNet model architecture from the\n",
    "#     `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "#     \"\"\"\n",
    "#     model = AlexNet(**kwargs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# num_classes_base = 34\n",
    "# num_classes_holdout = 34\n",
    "\n",
    "# Load model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 34)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = LeNet()\n",
    "# model = AlexNet3232()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this cell now that we can train using pytorch-classifcation/cbas.py: \n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# epochs = 20\n",
    "# print_every = 100\n",
    "# steps_per_epoch = int(base_train.__len__() / 4)\n",
    "\n",
    "# for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(base_train_loader, 0):\n",
    "#         # get the inputs\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # wrap them in Variable\n",
    "#         inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.data[0]\n",
    "#         if i % print_every == (print_every-1):    # print every 2000 mini-batches\n",
    "#             print('epoch[%d/%d, %5d/%d], loss: %.3f' %\n",
    "#                   (epoch + 1, epochs, i + 1, steps_per_epoch, running_loss / print_every))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can get rid of this cell now that we can train using pytorch-classifcation/cbas.py: \n",
    "# import os \n",
    "\n",
    "# # Save model trained on cbas-LS base set:\n",
    "# weights_dir = './weights/'\n",
    "# if not os.path.exists(weights_dir):\n",
    "#     os.makedirs(weights_dir)\n",
    "# # torch.save(model, os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./pytorch-classification\")\n",
    "sys.path.append(\"./pytorch-classification/models\")\n",
    "sys.path.append(\"./pytorch-classification/models/cifar/\")\n",
    "import os\n",
    "import cbas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import models.cifar as models\n",
    "\n",
    "def load_model(path, model=None): \n",
    "    gpu_id = \"0\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"use_cuda: \", use_cuda)\n",
    "\n",
    "    if model is None: \n",
    "        model = models.alexnet(num_classes=34)\n",
    "\n",
    "    if use_cuda:\n",
    "        print(\"Using GPU\")\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        model = model.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    else: \n",
    "        print(\"Using CPU (no GPU)\")\n",
    "        \n",
    "    checkpoint = torch.load(path)\n",
    "    #print(checkpoint.keys())\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model, use_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda:  True\n",
      "Using GPU\n",
      "True DataParallel(\n",
      "  (module): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(5, 5))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (classifier): Linear(in_features=256, out_features=34, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Grab weights from my google drive: https://drive.google.com/drive/u/0/folders/1BFYG5soBG6vBV7RC00NjadPYdCapWLgb\n",
    "# (use your berkeley google account)\n",
    "model, use_cuda = load_model('./pytorch-classification/alexnet_300epochs/model_best.pth.tar')\n",
    "print(use_cuda, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \n",
    "    def __init__(self, model=None, embed_layer=None, embed_size=256, transform=None):\n",
    "        self.embed_size = embed_size\n",
    "        self.transforms = transform\n",
    "        \n",
    "        if model is None: \n",
    "            self.model = models.alexnet(pretrained=True)\n",
    "            self.embed_layer = self.model.features\n",
    "        else: \n",
    "            if embed_layer is None: \n",
    "                raise ValueError(\"Need to specify embed_layer if you pass in a model to FeatureExtractor!\")\n",
    "            self.model = model\n",
    "            self.embed_layer = embed_layer\n",
    "        \n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            \n",
    "        # Set model to eval mode so any train-specific things like dropout, etc. don't run:\n",
    "        self.model.eval()\n",
    "    \n",
    "    def embed(self, img):\n",
    "        \"\"\"\n",
    "        project a PIL image into embedded feature space, and return that vector as an np array\n",
    "        \"\"\"\n",
    "        a = self.transforms(img)\n",
    "        image = Variable(a)\n",
    "        image = image.unsqueeze(0)\n",
    "        if self.cuda: image.cuda()\n",
    "        \n",
    "        embedding = torch.zeros(self.embed_size)\n",
    "        def copy_embedding(m, i, o):\n",
    "            if len(o.size()) > 2:\n",
    "                o = o.view(o.size(0), -1)\n",
    "            embedding.copy_(o.data)\n",
    "            \n",
    "        h = self.embed_layer.register_forward_hook(copy_embedding)\n",
    "        h_x = self.model(image)\n",
    "        h.remove()\n",
    "        return embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# # Load model pre-trained on cbas-LS base set: \n",
    "# weights_dir = './weights/'\n",
    "# if not os.path.exists(weights_dir):\n",
    "#     os.makedirs(weights_dir)\n",
    "\n",
    "# model = torch.load(os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    embedder = FeatureExtractor(model=model, transform=transform, embed_layer=model.module.features)\n",
    "else:\n",
    "    embedder = FeatureExtractor(model=model, transform=transform, embed_layer=model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,) [2.6054916e-01 0.0000000e+00 1.0512331e-01 1.4563650e+00 6.4134377e-01\n",
      " 0.0000000e+00 0.0000000e+00 7.0150042e-01 6.5649128e-01 1.2014824e+00\n",
      " 3.9627203e-01 0.0000000e+00 7.3409997e-02 0.0000000e+00 3.5294607e-01\n",
      " 1.9359577e-01 1.2204086e+00 1.0499220e+00 6.9113761e-01 3.8602275e-01\n",
      " 0.0000000e+00 0.0000000e+00 1.8278308e-02 3.1387180e-02 6.4763173e-02\n",
      " 2.1722837e-01 8.4295467e-02 7.5883573e-01 4.5470601e-01 0.0000000e+00\n",
      " 9.9060563e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 5.2719552e-02 5.1462817e-01 3.0826843e-01 5.2347261e-01 7.3208290e-01\n",
      " 7.7685797e-01 6.6841614e-01 7.1570411e-02 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 8.8929415e-02 0.0000000e+00 1.8442227e+00\n",
      " 4.9732074e-01 8.6772782e-01 0.0000000e+00 0.0000000e+00 9.9176395e-01\n",
      " 1.0321523e+00 6.4503139e-01 3.8436830e-02 1.3690810e+00 1.4549842e-01\n",
      " 0.0000000e+00 3.4849077e-02 0.0000000e+00 0.0000000e+00 4.9966094e-01\n",
      " 1.8069856e+00 3.9278030e-01 7.3927468e-01 0.0000000e+00 5.1306415e-01\n",
      " 0.0000000e+00 1.8066674e+00 3.3023733e-01 0.0000000e+00 6.1402678e-01\n",
      " 0.0000000e+00 1.5856877e-01 1.0917088e+00 3.5189006e-01 5.2565676e-01\n",
      " 8.7889135e-01 0.0000000e+00 0.0000000e+00 7.7531451e-01 1.0772613e+00\n",
      " 0.0000000e+00 1.2997933e-01 6.2068093e-01 1.4653659e-01 0.0000000e+00\n",
      " 0.0000000e+00 1.4789723e+00 3.4419146e-01 0.0000000e+00 1.3793627e+00\n",
      " 5.1475577e-02 5.0211215e-01 7.5593695e-02 0.0000000e+00 3.9038366e-01\n",
      " 9.6782851e-01 4.9343482e-01 2.4553764e+00 0.0000000e+00 8.3951354e-01\n",
      " 1.6398680e+00 1.1428531e+00 3.5012554e-02 1.8742944e+00 9.2988729e-01\n",
      " 5.6648344e-02 1.7537546e+00 0.0000000e+00 1.5588734e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 4.3776137e-01 8.2616128e-02 0.0000000e+00 2.5153434e-01 1.2979987e-01\n",
      " 8.9069411e-02 0.0000000e+00 1.7071262e-02 2.0955651e+00 1.9517312e+00\n",
      " 1.2885249e-01 2.5440332e-01 0.0000000e+00 1.4944756e-01 0.0000000e+00\n",
      " 1.6579716e+00 1.0544462e-01 1.4675368e+00 0.0000000e+00 1.3645542e-01\n",
      " 0.0000000e+00 2.4190482e-01 7.3913258e-01 0.0000000e+00 6.3665515e-01\n",
      " 0.0000000e+00 1.6449394e+00 3.6191940e-01 0.0000000e+00 1.1510203e+00\n",
      " 1.1305976e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5072020e-01\n",
      " 0.0000000e+00 1.3352187e-01 0.0000000e+00 5.3785992e-01 1.5330514e+00\n",
      " 6.6267796e-02 4.1041538e-02 0.0000000e+00 9.0103257e-01 5.4209685e-01\n",
      " 6.9863397e-01 1.8504238e-01 4.6052969e-01 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 2.1567383e-01 1.2805901e+00 5.5354369e-01 1.6464403e+00\n",
      " 3.7424463e-01 0.0000000e+00 0.0000000e+00 2.1532100e-01 7.8605697e-02\n",
      " 3.4130225e-01 2.7575222e-01 0.0000000e+00 0.0000000e+00 1.3637531e+00\n",
      " 0.0000000e+00 1.4295642e-01 1.9704220e-01 0.0000000e+00 1.3235162e-01\n",
      " 1.3260365e-01 1.2504311e+00 0.0000000e+00 4.5274699e-01 6.0810894e-01\n",
      " 6.5223861e-01 0.0000000e+00 9.2413568e-01 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 1.9062860e+00 0.0000000e+00 6.5383598e-02\n",
      " 3.7795168e-01 7.3295844e-01 7.6904893e-05 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 4.2009550e-01 1.1530702e-01 4.4674265e-01 0.0000000e+00\n",
      " 1.9040217e-01 0.0000000e+00 0.0000000e+00 1.1353031e+00 1.6767028e-01\n",
      " 2.4340382e-01 9.5832467e-01 1.9080677e+00 7.4202722e-01 7.8374565e-01\n",
      " 1.7459313e+00 1.6599712e+00 1.7420667e+00 8.3202046e-01 1.0944723e+00\n",
      " 0.0000000e+00 0.0000000e+00 2.6649153e-01 1.3076819e+00 0.0000000e+00\n",
      " 4.9839411e-02 1.0043280e+00 1.7644060e-01 1.7195113e+00 0.0000000e+00\n",
      " 1.7364889e-01 3.6934018e-01 1.3673797e-02 0.0000000e+00 1.4220954e+00\n",
      " 3.8117617e-01 0.0000000e+00 7.6692647e-01 1.3539470e+00 2.2310867e+00\n",
      " 0.0000000e+00 5.4792099e-02 0.0000000e+00 1.8221638e+00 0.0000000e+00\n",
      " 2.5241035e-01]\n",
      "(256,) [0.05294427 0.         0.9827287  0.05508612 0.04556049 0.578705\n",
      " 0.23143795 0.         0.         0.         0.03120963 0.\n",
      " 0.2616319  0.         0.07498985 0.3463482  0.3392625  0.07091939\n",
      " 0.10300451 0.17137513 0.         0.425942   0.20665091 0.14249519\n",
      " 0.12767512 0.05386529 0.         0.14799236 0.5193986  0.58367914\n",
      " 0.00958752 0.         0.09635161 0.06654869 0.         0.460028\n",
      " 0.6075424  0.04255909 0.09900118 0.16791302 0.9916736  0.\n",
      " 0.17046367 0.         0.65176934 0.         0.21151054 0.5507596\n",
      " 0.26812816 0.17075391 0.23096211 1.0620521  0.         0.\n",
      " 0.         0.97859776 0.49443406 0.24050066 0.45855448 0.\n",
      " 0.4554443  0.         0.         0.         0.23722914 0.\n",
      " 0.         0.29862866 0.1595145  0.15717225 0.         0.12222545\n",
      " 0.14901511 0.47576803 0.03744084 0.         0.0367988  0.7856201\n",
      " 0.         0.01540045 0.19121166 0.18701693 0.3037008  0.20373967\n",
      " 0.6072214  0.         0.26578215 0.2207376  0.         0.64694506\n",
      " 0.06816177 0.48054135 0.16137424 0.34685332 0.11909225 0.\n",
      " 0.33853886 0.14617738 0.         0.         0.55407935 0.10722755\n",
      " 0.13606751 0.         0.         0.         0.         0.17976569\n",
      " 1.0337663  0.         0.18257067 0.4243259  0.11158025 0.\n",
      " 0.         0.6756177  0.27696157 0.10190932 0.         0.\n",
      " 0.49971247 0.35731477 0.5449575  0.07576016 0.         0.\n",
      " 0.37720346 0.34862682 0.         0.13487214 0.08702219 0.12210155\n",
      " 0.6942416  0.         0.05904891 0.34537083 0.2691601  0.\n",
      " 0.         0.         0.9266068  0.         0.26501104 0.77652013\n",
      " 0.         0.27135485 0.08626691 0.01202761 0.         0.0386272\n",
      " 0.45287523 0.05694136 0.         0.42670274 0.         0.21317503\n",
      " 0.32405064 0.         0.         0.         0.10380938 0.13194376\n",
      " 0.16613133 0.05623747 0.2672416  0.         0.         0.4540422\n",
      " 0.1166634  0.4428077  0.         0.         0.         0.19743556\n",
      " 0.         0.05745045 0.         0.         0.20768744 0.\n",
      " 0.         0.5142629  0.         0.06109221 0.19679804 0.326888\n",
      " 0.02389773 0.05081737 0.54081583 0.         0.         0.08279042\n",
      " 0.62328297 0.47440416 0.         0.8149603  0.29084533 0.\n",
      " 0.1423167  0.5432434  0.         0.13159247 0.         0.8859383\n",
      " 0.7039255  0.         0.83260316 0.         0.0565632  0.04751353\n",
      " 1.4260991  0.3989508  0.45997077 0.5277741  0.8436446  0.33949605\n",
      " 0.8866571  0.9999225  0.         0.04493075 0.         0.4183928\n",
      " 0.40660906 0.         0.6386906  0.72182244 0.08085969 0.\n",
      " 0.         0.         0.         0.         0.4108502  0.48040858\n",
      " 0.3288148  0.0275306  0.         0.08844092 0.         0.\n",
      " 0.0922857  0.         0.4766397  0.         0.         0.\n",
      " 0.11190125 0.64984757 0.05871008 0.17694414 0.         0.\n",
      " 0.13675462 0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get some random training images\n",
    "dataiter = iter(base_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Test embedding a couple of images. For Alexnet (32x32 images) this should be a 256-d vector: \n",
    "img = Image.open('../images/cbas34_train/airplane/156356.jpg')\n",
    "emb = embedder.embed(img)\n",
    "print(emb.shape, emb)\n",
    "\n",
    "img = Image.open('../images/cbas34_train/cow/69943.jpg')\n",
    "emb = embedder.embed(img)\n",
    "print(emb.shape, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Embedding Results\n",
    "Now let's compute cosine similarities for one airplane image against all airplanes, and then similarities for airplane against several other non-airplane categories. The average cosine similarity for airplane vs. airplane should be higher than that for airplane-vs-non-airplanes, if we are computing good embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  airplane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'airplane' intra-category similarity: 0.5554724133849144\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'cow'...\n",
      "Hack: converting ../images/cbas34_train/airplane/156618.jpg to rgb...\n",
      "average airplane cos-similarity vs 'cow': 0.42189421662688253\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'banana'...\n",
      "average airplane cos-similarity vs 'banana': 0.3343877656400204\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'sink'...\n",
      "average airplane cos-similarity vs 'sink': 0.39150713973641393\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'carrot'...\n",
      "Hack: converting ../images/cbas34_train/airplane/1363244.jpg to rgb...\n",
      "Hack: converting ../images/cbas34_train/airplane/160229.jpg to rgb...\n",
      "average airplane cos-similarity vs 'carrot': 0.3539051051974297\n",
      "\n",
      "Testing inter-category cosine similarities for 'airplane' vs 'umbrella'...\n",
      "Hack: converting ../images/cbas34_train/airplane/1363538.jpg to rgb...\n",
      "average airplane cos-similarity vs 'umbrella': 0.4348770597040653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.random.seed(55)\n",
    "\n",
    "def get_cos_similarities(base_img_path, other_image_paths):\n",
    "    img = Image.open(base_img_path)\n",
    "    # Work arround issue of some of the train images not being RGB, (they won't go through the CNN): \n",
    "    if img.mode != \"RGB\":\n",
    "        print(\"Hack: converting {} to rgb...\".format(base_img_path))\n",
    "        rgbimg = Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    base_embedding = embedder.embed(img)\n",
    "    similarities = np.zeros((len(other_image_paths)))\n",
    "    \n",
    "    for idx,img_path in enumerate(other_image_paths):\n",
    "        #print(\"idx: \", idx, img_path)\n",
    "        embedding = None\n",
    "        img = Image.open(img_path)\n",
    "        try:\n",
    "            embedding = embedder.embed(img)\n",
    "        except:\n",
    "            print(\"error on image #\", idx)\n",
    "        if embedding is not None: \n",
    "#             cos_sim = cosine_similarity(base_embedding, embedding)\n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            if use_cuda:\n",
    "                cos_sim = cos(torch.Tensor(base_embedding).cuda().unsqueeze(0), torch.Tensor(embedding).cuda().unsqueeze(0))\n",
    "            else: \n",
    "                cos_sim = cos(torch.Tensor(base_embedding).unsqueeze(0), torch.Tensor(embedding).unsqueeze(0))\n",
    "            similarities[idx] = cos_sim.cpu().numpy()\n",
    "    return similarities\n",
    "\n",
    "def avg_category_similarities(dataset_path, cat1, cat2, sample_size=50):\n",
    "    \"\"\"\n",
    "    Compute avereage similarities between all imgs in categories cat1 an cat2\n",
    "    \"\"\"\n",
    "    cat1_imgs = glob.glob(dataset_path.format(cat1)+'*.jpg')\n",
    "    cat2_imgs = glob.glob(dataset_path.format(cat2)+'*.jpg')\n",
    "    # Sample a limited size to reduce compute time: \n",
    "    size1, size2 = min(len(cat1_imgs), sample_size), min(len(cat2_imgs), sample_size)\n",
    "    cat1_imgs = np.random.choice(cat1_imgs, size=size1)\n",
    "    cat2_imgs = np.random.choice(cat2_imgs, size=size2)\n",
    "    sims = np.zeros((size1, size2))\n",
    "    #print(\"similarites shape: \", sims.shape)\n",
    "    for i,img1 in enumerate(cat1_imgs):\n",
    "        sims[i,:] = get_cos_similarities(img1, cat2_imgs).T\n",
    "        pass\n",
    "    return sims\n",
    "\n",
    "\n",
    "base_cat = \"airplane\"\n",
    "other_cats = [\"cow\", \"banana\", \"sink\", \"carrot\", \"umbrella\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbas34_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbas34_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))\n",
    "\n",
    "\n",
    "## NOTE: the error messages here are mostly due to some of the training \n",
    "## images in cbas-34 being 1 channel instead of 3. We should regenerate the \n",
    "## cbas set to be 3-channel to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  banana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'banana' intra-category similarity: 0.5981612083077431\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'cow'...\n",
      "average banana cos-similarity vs 'cow': 0.39726638778448103\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'airplane'...\n",
      "average banana cos-similarity vs 'airplane': 0.346548106855154\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'sink'...\n",
      "average banana cos-similarity vs 'sink': 0.4291816424369812\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'carrot'...\n",
      "average banana cos-similarity vs 'carrot': 0.48758098617196083\n",
      "\n",
      "Testing inter-category cosine similarities for 'banana' vs 'umbrella'...\n",
      "average banana cos-similarity vs 'umbrella': 0.4026957649409771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"banana\"\n",
    "other_cats = [\"cow\", \"airplane\", \"sink\", \"carrot\", \"umbrella\"]\n",
    "\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbas34_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbas34_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we try embedding images from classes in the holdout set?\n",
    "\n",
    "In the preceding cells we did a sanity check to verify that embeddings made some kind of sense, but those were all embeddings the network was trained on. What about when we embed images for classes the network was not trained on? Let's try it out by doing similar comparisons for categories in the cbas-LS set (i.e., holdout set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  mouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'mouse' intra-category similarity: 0.4843701311588287\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'orange'...\n",
      "average mouse cos-similarity vs 'orange': 0.4362938178360462\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'suitcase'...\n",
      "average mouse cos-similarity vs 'suitcase': 0.44095144353508947\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'toilet'...\n",
      "average mouse cos-similarity vs 'toilet': 0.49534857615828515\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'elephant'...\n",
      "average mouse cos-similarity vs 'elephant': 0.4134134413480759\n",
      "\n",
      "Testing inter-category cosine similarities for 'mouse' vs 'giraffe'...\n",
      "average mouse cos-similarity vs 'giraffe': 0.41006553488373754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"mouse\"\n",
    "other_cats = [\"orange\", \"suitcase\", \"toilet\", \"elephant\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  orange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'orange' intra-category similarity: 0.6107145872890949\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'hotdog'...\n",
      "average orange cos-similarity vs 'hotdog': 0.5331870565474033\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'suitcase'...\n",
      "average orange cos-similarity vs 'suitcase': 0.3608613192200661\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'laptop'...\n",
      "average orange cos-similarity vs 'laptop': 0.40409152723550795\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'scissors'...\n",
      "average orange cos-similarity vs 'scissors': 0.43913503798246384\n",
      "\n",
      "Testing inter-category cosine similarities for 'orange' vs 'giraffe'...\n",
      "average orange cos-similarity vs 'giraffe': 0.379066547191143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"orange\"\n",
    "other_cats = [\"hotdog\", \"suitcase\", \"laptop\", \"scissors\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing intra-category cosine similarities for category:  pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 'pizza' intra-category similarity: 0.5697632890284061\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'hotdog'...\n",
      "average pizza cos-similarity vs 'hotdog': 0.5485961789369583\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'remote'...\n",
      "average pizza cos-similarity vs 'remote': 0.4786675609171391\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'laptop'...\n",
      "average pizza cos-similarity vs 'laptop': 0.46263093419075013\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'stopsign'...\n",
      "average pizza cos-similarity vs 'stopsign': 0.4123535647749901\n",
      "\n",
      "Testing inter-category cosine similarities for 'pizza' vs 'giraffe'...\n",
      "average pizza cos-similarity vs 'giraffe': 0.39758804389238356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cat = \"pizza\"\n",
    "other_cats = [\"hotdog\", \"remote\", \"laptop\", \"stopsign\", \"giraffe\"]\n",
    "\n",
    "print(\"Testing intra-category cosine similarities for category: \", base_cat)\n",
    "intra_cat_sims = avg_category_similarities('../images/cbasLS_train/{}/', base_cat, base_cat)\n",
    "print(\n",
    "    \"average '{}' intra-category similarity: {}\\n\".format(\n",
    "        base_cat\n",
    "        , np.average(intra_cat_sims)\n",
    "    ))\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing inter-category cosine similarities for '{}' vs '{}'...\"\\\n",
    "          .format(base_cat, other_cat))\n",
    "    inter_cat_sims = avg_category_similarities(\n",
    "        '../images/cbasLS_train/{}/', base_cat, other_cat\n",
    "    )\n",
    "    print(\"average {} cos-similarity vs '{}': {}\\n\".format(\n",
    "        base_cat, other_cat, np.average(inter_cat_sims)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
