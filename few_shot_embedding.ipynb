{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../PythonAPI')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from cbas import CBAS\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cbas import CBAS\n",
    "import cbas_construction_utils as ccu\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "idToName:  80 {'1': 'person', '2': 'bicycle', '3': 'car', '4': 'motorcycle', '5': 'airplane', '6': 'bus', '7': 'train', '8': 'truck', '9': 'boat', '10': 'traffic light', '11': 'fire hydrant', '13': 'stop sign', '14': 'parking meter', '15': 'bench', '16': 'bird', '17': 'cat', '18': 'dog', '19': 'horse', '20': 'sheep', '21': 'cow', '22': 'elephant', '23': 'bear', '24': 'zebra', '25': 'giraffe', '27': 'backpack', '28': 'umbrella', '31': 'handbag', '32': 'tie', '33': 'suitcase', '34': 'frisbee', '35': 'skis', '36': 'snowboard', '37': 'sports ball', '38': 'kite', '39': 'baseball bat', '40': 'baseball glove', '41': 'skateboard', '42': 'surfboard', '43': 'tennis racket', '44': 'bottle', '46': 'wine glass', '47': 'cup', '48': 'fork', '49': 'knife', '50': 'spoon', '51': 'bowl', '52': 'banana', '53': 'apple', '54': 'sandwich', '55': 'orange', '56': 'broccoli', '57': 'carrot', '58': 'hot dog', '59': 'pizza', '60': 'donut', '61': 'cake', '62': 'chair', '63': 'couch', '64': 'potted plant', '65': 'bed', '67': 'dining table', '70': 'toilet', '72': 'tv', '73': 'laptop', '74': 'mouse', '75': 'remote', '76': 'keyboard', '77': 'cell phone', '78': 'microwave', '79': 'oven', '80': 'toaster', '81': 'sink', '82': 'refrigerator', '84': 'book', '85': 'clock', '86': 'vase', '87': 'scissors', '88': 'teddy bear', '89': 'hair drier', '90': 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# initialize COCO api for instance annotations and category info\n",
    "cbas80=CBAS('../annotations/{}.json'.format('cbas80'))\n",
    "\n",
    "# Get category index so we can go from image ids to category names\n",
    "idToName={}\n",
    "for c in cbas80.dataset['categories']:\n",
    "    idToName[str(c['id'])]=c['name']\n",
    "\n",
    "print(\"idToName: \", len(idToName.items()), idToName)\n",
    "# base set is cbas34, and holdout set is cbas80-cbas34\n",
    "# holdout is cbas80 - cbas34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cbas_34 base and holdout (AKA \"target\" classes) sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "base_train = torchvision.datasets.ImageFolder(root='../images/cbas34_train', transform=transform)\n",
    "base_train_loader = torch.utils.data.DataLoader(base_train, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "base_valid = torchvision.datasets.ImageFolder(root='../images/cbas34_val', transform=transform)\n",
    "base_valid_loader = torch.utils.data.DataLoader(base_valid, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# get index for curriculum sampling\n",
    "id2idx = {}\n",
    "for i,img in enumerate(base_train.imgs):\n",
    "    img_id_str = img[0].split('/')[4].split('.')[0]\n",
    "    id2idx[img_id_str] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['root', 'imgs', 'classes', 'class_to_idx', 'transform', 'target_transform', 'loader'])\n",
      "('../images/cbas34_train/airplane/1363089.jpg', 0)\n",
      "Base classes (34 total): \n",
      "['airplane', 'backpack', 'banana', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'car', 'carrot', 'chair', 'clock', 'cow', 'cup', 'donut', 'fork', 'handbag', 'horse', 'kite', 'knife', 'person', 'pottedplant', 'sheep', 'sink', 'skateboard', 'spoon', 'surfboard', 'tennisracket', 'trafficlight', 'truck', 'umbrella', 'vase']\n",
      "base_train size:  51000\n",
      "steps per epoch:  12750\n"
     ]
    }
   ],
   "source": [
    "print(base_train.__dict__.keys())\n",
    "print(base_train.imgs[3])\n",
    "print(\"Base classes ({} total): \".format(len(base_train.classes)))\n",
    "print(base_train.classes)\n",
    "\n",
    "print(\"base_train size: \", base_train.__len__())\n",
    "print(\"steps per epoch: \", int(base_train.__len__() / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the holdout categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = COCO.loadCats(COCO.getCatIds())\n",
    "# nms=[cat['name'] for cat in all_categories]\n",
    "# print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "# Get all categories: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network that learns to predict images from base_set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "__all__ = ['alexnet3232']\n",
    "\n",
    "\n",
    "class AlexNet3232(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=34):\n",
    "        super(AlexNet3232, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet3232(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# num_classes_base = 34\n",
    "# num_classes_holdout = 34\n",
    "\n",
    "# Load model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 34)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = LeNet()\n",
    "model = AlexNet3232()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "# print_every = 100\n",
    "# steps_per_epoch = int(base_train.__len__() / 4)\n",
    "\n",
    "# for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(base_train_loader, 0):\n",
    "#         # get the inputs\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # wrap them in Variable\n",
    "#         inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.data[0]\n",
    "#         if i % print_every == (print_every-1):    # print every 2000 mini-batches\n",
    "#             print('epoch[%d/%d, %5d/%d], loss: %.3f' %\n",
    "#                   (epoch + 1, epochs, i + 1, steps_per_epoch, running_loss / print_every))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \n",
    "    def __init__(self, model=None, embed_layer=None, embed_size=256, transform=None):\n",
    "        self.embed_size = embed_size\n",
    "        self.transforms = transform\n",
    "        \n",
    "        if model is None: \n",
    "            self.model = models.alexnet(pretrained=True)\n",
    "            self.embed_layer = self.model.features\n",
    "        else: \n",
    "            if embed_layer is None: \n",
    "                raise ValueError(\"Need to specify embed_layer if you pass in a model to FeatureExtractor!\")\n",
    "            self.model = model\n",
    "            self.embed_layer = embed_layer\n",
    "        \n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            \n",
    "        # Set model to eval mode so any train-specific things like dropout, etc. don't run:\n",
    "        self.model.eval()\n",
    "    \n",
    "    def embed(self, img):\n",
    "        \"\"\"\n",
    "        project a PIL image into embedded feature space, and return that vector as an np array\n",
    "        \"\"\"\n",
    "        a = self.transforms(img)\n",
    "        image = Variable(a)\n",
    "        image = image.unsqueeze(0)\n",
    "        if self.cuda: image.cuda()\n",
    "        \n",
    "        embedding = torch.zeros(self.embed_size)\n",
    "        def copy_embedding(m, i, o):\n",
    "            if len(o.size()) > 2:\n",
    "                o = o.view(o.size(0), -1)\n",
    "            embedding.copy_(o.data)\n",
    "            \n",
    "        h = self.embed_layer.register_forward_hook(copy_embedding)\n",
    "        h_x = self.model(image)\n",
    "        h.remove()\n",
    "        return embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "weights_dir = './weights/'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.makedirs(weights_dir)\n",
    "# torch.save(model, os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))\n",
    "model = torch.load(os.path.join(weights_dir, 'alexnet_cbas34_baseset_20_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = FeatureExtractor(model=model, transform=transform, embed_layer=model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/cs189/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get some random training images\n",
    "dataiter = iter(base_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Test embedding one image into embedding space. For Alexnet (32x32 images) this should be a 256-d vector: \n",
    "img = Image.open('../images/cbas34_train/airplane/156356.jpg')\n",
    "print(embedder.embed(img).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results\n",
    "Now let's compute cosine similarities for one airplane image against all airplanes, and then similarities for airplane against several other non-airplane categories. The average cosine similarity for airplane vs. airplane should be higher than that for airplane-vs-non-airplanes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing airplane cosine similarity against airplanes and against:  ['cow', 'banana', 'sink', 'carrot', 'umbrella']\n",
      "Testing cosine similarities for base airplane against other airplane images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/anaconda3/envs/cs189/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on image # 81\n",
      "error on image # 91\n",
      "error on image # 316\n",
      "error on image # 330\n",
      "error on image # 548\n",
      "error on image # 872\n",
      "error on image # 1027\n",
      "error on image # 1115\n",
      "error on image # 1171\n",
      "error on image # 1347\n",
      "error on image # 1415\n",
      "error on image # 1442\n",
      "error on image # 1481\n",
      "average airplane similarity vs other airplanes:  0.41849434301505484\n",
      "\n",
      "Testing cosine similarities for base airplane against NON-airplane images from category: cow...\n",
      "error on image # 689\n",
      "average airplane cos-similarity vs 'cow': 0.13348377155760924\n",
      "\n",
      "Testing cosine similarities for base airplane against NON-airplane images from category: banana...\n",
      "average airplane cos-similarity vs 'banana': 0.11696031159038345\n",
      "\n",
      "Testing cosine similarities for base airplane against NON-airplane images from category: sink...\n",
      "average airplane cos-similarity vs 'sink': 0.18486352485930546\n",
      "\n",
      "Testing cosine similarities for base airplane against NON-airplane images from category: carrot...\n",
      "average airplane cos-similarity vs 'carrot': 0.09549224826383094\n",
      "\n",
      "Testing cosine similarities for base airplane against NON-airplane images from category: umbrella...\n",
      "error on image # 124\n",
      "error on image # 138\n",
      "error on image # 438\n",
      "error on image # 527\n",
      "error on image # 576\n",
      "error on image # 1014\n",
      "average airplane cos-similarity vs 'umbrella': 0.11811630738464494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "def get_cos_similarities(base_img_path, other_images_path):\n",
    "    img = Image.open(base_img_path)\n",
    "    base_embedding = embedder.embed(img)\n",
    "    other_imgs = glob.glob(other_images_path + '*.jpg')\n",
    "    similarities = np.zeros((len(other_imgs)))\n",
    "    \n",
    "    for idx,img_path in enumerate(other_imgs):\n",
    "        #print(\"idx: \", idx, img_path)\n",
    "        embedding = None\n",
    "        img = Image.open(img_path)\n",
    "        try:\n",
    "            embedding = embedder.embed(img)\n",
    "        except:\n",
    "            print(\"error on image #\", idx)\n",
    "        if embedding is not None: \n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            cos_sim = cos(torch.Tensor(base_embedding).unsqueeze(0), torch.Tensor(embedding).unsqueeze(0))\n",
    "            similarities[idx] = cos_sim.numpy()\n",
    "    return similarities\n",
    "\n",
    "other_cats = [\"cow\", \"banana\", \"sink\", \"carrot\", \"umbrella\"]\n",
    "print(\"Testing airplane cosine similarity against airplanes and against: \", other_cats)\n",
    "\n",
    "print(\"Testing cosine similarities for base airplane against other airplane images...\")\n",
    "airplane_vs_airplane_similarities = get_cos_similarities(\n",
    "    '../images/cbas34_train/airplane/156356.jpg'\n",
    "    , '../images/cbas34_train/airplane/'\n",
    ")\n",
    "print(\"average airplane similarity vs other airplanes: \", np.average(airplane_vs_airplane_similarities))\n",
    "print()\n",
    "\n",
    "for other_cat in other_cats: \n",
    "    print(\"Testing cosine similarities for base airplane against NON-airplane images from category: {}...\".format(other_cat))\n",
    "    airplane_vs_other_similarities = get_cos_similarities(\n",
    "        '../images/cbas34_train/airplane/156356.jpg'\n",
    "        , '../images/cbas34_train/{}/'.format(other_cat)\n",
    "    )\n",
    "    print(\"average airplane cos-similarity vs '{}': {}\".format(other_cat, np.average(airplane_vs_other_similarities)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
