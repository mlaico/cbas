{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the below statement evaluates to false, please remove the .cuda() method calls on each Tensor. This statement determines if your GPU can be utilized for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import sys\n",
    "sys.path.append(\"./pytorch-classification\")\n",
    "sys.path.append(\"./pytorch-classification/models\")\n",
    "sys.path.append(\"./pytorch-classification/models/cifar/\")\n",
    "import os\n",
    "import cbas\n",
    "import torch.backends.cudnn as cudnn\n",
    "import models.cifar as models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "Next, we load our dataset (CBAS-34) into ImageFolder and DataLoader classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='../images/cbas34_val', transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "trainset = torchvision.datasets.ImageFolder(root='../images/cbas34_train', transform=transform)\n",
    "classes = trainset.classes\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />  \n",
    "\n",
    "<br />  \n",
    "\n",
    "In order to determine the true loss ordering associated with each epoch we need to override the Sampler class found in torch.utils.data.sampler. In particular, we need to be able to attain the order of indices sampled throughout training (note the self.random_list class variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampler(Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        self.random_list = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.random_list = torch.randperm(len(self.data_source)).tolist()\n",
    "        return iter(self.random_list)\n",
    "\n",
    "    def get_idx(self):\n",
    "        return self.random_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "\n",
    "class MyWeightedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, weights, num_samples, replacement=True):\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "        self.random_list = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        ret = torch.multinomial(self.weights, self.num_samples, self.replacement)\n",
    "        self.random_list = ret.numpy().tolist()\n",
    "        return iter(ret)\n",
    "\n",
    "    def get_idx(self):\n",
    "        return self.random_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  <br />  \n",
    "    \n",
    "<br />\n",
    "\n",
    "\n",
    "Here we define our method of determining a difficulty metric based on loss from the previous epoch. This involves utilizing 'get_idx' from the Sampler classes defined above (overriding methods defined in torch.utils.data.sampler) to determine the true ordering of images (relative to the original dataset) and definining a set of weights for our 'MyWeightedSampler' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normal_weights(losses, mu=None):\n",
    "    mu, var = mu if mu else np.mean(losses), np.var(losses)\n",
    "    return (1/(np.sqrt(np.pi*2*var)))*np.exp(-((losses-mu)**2)/(2*var))\n",
    "\n",
    "\n",
    "def real_time(training_set, model, loss_fn, optimizer, deviations):\n",
    "    \"\"\"\n",
    "    training_set: class type 'torchvision.datasets.ImageFolder'\n",
    "\n",
    "    deviations: a sequence of standard deviations scalars to be applied to the sampling distribution's\n",
    "    mean to determine the probability of sampling and image with a given loss value. If set to [0...0],\n",
    "    the probability of sampling each image (based on loss value) will be determined by the normal\n",
    "    distribution's pdf. If deviation = -1, probability will be dictated by a normal with shifted mean\n",
    "    mean(loss) -1*std(loss). This in effect allows us to shift the difficulty of training images over\n",
    "    each epoch. Images are sampled with replacement, so we can shift the focus from easy to hard. For\n",
    "    example: [-1, 0, 1] samples from a normal distribution centered at mean(loss) -1*std(loss),\n",
    "    mean(loss), then mean(loss) + 1*std(loss) for the training epochs.\n",
    "\n",
    "    Note: number of epochs == len(deviations) + 1 (+1 for the initial training epoch)\n",
    "    \"\"\"\n",
    "\n",
    "    def real_time_curriculum(sampler, loader, net, criterion, optimizer):\n",
    "        orderings = []\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            try:\n",
    "                numpy_labels = labels.numpy()\n",
    "            except:\n",
    "                numpy_labels = labels.data.numpy()\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            try:\n",
    "                numpy_outputs = outputs.cpu().numpy()\n",
    "            except:\n",
    "                numpy_outputs = outputs.cpu().data.numpy()\n",
    "            log_probs = -np.log(np.exp(numpy_outputs)\n",
    "                                / np.reshape(np.sum(np.exp(numpy_outputs), axis=1), (numpy_labels.shape[0], 1)))\n",
    "            orderings += log_probs[:, numpy_labels].tolist()[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('%5d loss: %.3f' %\n",
    "                      (i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "        idx = np.argsort(np.array(sampler.get_idx()))\n",
    "        culmulative_orderings = np.array(orderings)[idx]\n",
    "        return culmulative_orderings\n",
    "\n",
    "    my_sampler = MySampler(training_set)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        training_set, batch_size=4, shuffle=False, sampler=my_sampler, num_workers=4)\n",
    "\n",
    "    print(\"epoch #1\")\n",
    "    real_time_curr = \\\n",
    "        real_time_curriculum(my_sampler, trainloader, model, loss_fn, optimizer)\n",
    "    epoch = 1\n",
    "    num_samples = real_time_curr.shape[0]\n",
    "\n",
    "    for deviation in deviations:\n",
    "        epoch += 1\n",
    "        print(\"epoch #%d\" % epoch)\n",
    "        weights = normal_weights(real_time_curr, np.mean(real_time_curr) + deviation * np.std(real_time_curr))\n",
    "        weight_denom = np.sum(weights)\n",
    "        weight_denom = weight_denom if weight_denom > (1/1e30) else (1/1e30)\n",
    "        weights = weights / weight_denom\n",
    "        sampler = MyWeightedSampler(weights, num_samples, replacement=True)\n",
    "        real_time_curriculum_loader = \\\n",
    "            torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=False, sampler=sampler, num_workers=4)\n",
    "        real_time_curr = \\\n",
    "            real_time_curriculum(sampler, real_time_curriculum_loader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "Here we define a training loop for our comparison models (with no curriculum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_curriculum(net, loss_fn, optimizer, epochs):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "### Load pre-trained models: \n",
    "To test our curriculum strategy we employ a variety of models to see how it interacts with different network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "real_time_alex = models.alexnet(num_classes=34).cuda()\n",
    "real_time_dile = models.dilenet(num_classes=34).cuda()\n",
    "real_time_lenet = models.lenet(num_classes=34).cuda()\n",
    "\n",
    "no_curriculum_alex = models.alexnet(num_classes=34).cuda()\n",
    "no_curriculum_dile = models.dilenet(num_classes=34).cuda()\n",
    "no_curriculum_lenet = models.lenet(num_classes=34).cuda()\n",
    "\n",
    "opt_real_alex = optim.SGD(real_time_alex.parameters(), lr=0.001, momentum=0.9)\n",
    "opt_real_dile = optim.SGD(real_time_dile.parameters(), lr=0.001, momentum=0.9)\n",
    "opt_real_lenet = optim.SGD(real_time_lenet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "opt_alex = optim.SGD(no_curriculum_alex.parameters(), lr=0.001, momentum=0.9)\n",
    "opt_dile = optim.SGD(no_curriculum_dile.parameters(), lr=0.001, momentum=0.9)\n",
    "opt_lenet = optim.SGD(no_curriculum_lenet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "Here we train alexnet, dilenet and lenet using our real-time loss based curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #1\n",
      " 2000 loss: 3.525\n",
      " 4000 loss: 3.432\n",
      " 6000 loss: 3.333\n",
      " 8000 loss: 3.291\n",
      "10000 loss: 3.266\n",
      "12000 loss: 3.206\n",
      "epoch #2\n",
      " 2000 loss: 2.983\n",
      " 4000 loss: 2.942\n",
      " 6000 loss: 2.900\n",
      " 8000 loss: 2.849\n",
      "10000 loss: 2.822\n",
      "12000 loss: 2.788\n",
      "epoch #3\n",
      " 2000 loss: 2.938\n",
      " 4000 loss: 2.886\n",
      " 6000 loss: 2.886\n",
      " 8000 loss: 2.851\n",
      "10000 loss: 2.795\n",
      "12000 loss: 2.792\n",
      "epoch #4\n",
      " 2000 loss: 2.785\n",
      " 4000 loss: 2.756\n",
      " 6000 loss: 2.729\n",
      " 8000 loss: 2.711\n",
      "10000 loss: 2.677\n",
      "12000 loss: 2.649\n",
      "epoch #5\n",
      " 2000 loss: 2.581\n",
      " 4000 loss: 2.563\n",
      " 6000 loss: 2.522\n",
      " 8000 loss: 2.507\n",
      "10000 loss: 2.483\n",
      "12000 loss: 2.424\n",
      "epoch #1\n",
      " 2000 loss: 3.371\n",
      " 4000 loss: 3.161\n",
      " 6000 loss: 3.045\n",
      " 8000 loss: 2.967\n",
      "10000 loss: 2.909\n",
      "12000 loss: 2.861\n",
      "epoch #2\n",
      " 2000 loss: 2.490\n",
      " 4000 loss: 2.416\n",
      " 6000 loss: 2.342\n",
      " 8000 loss: 2.288\n",
      "10000 loss: 2.199\n",
      "12000 loss: 2.121\n",
      "epoch #3\n",
      " 2000 loss: 2.494\n",
      " 4000 loss: 2.404\n",
      " 6000 loss: 2.381\n",
      " 8000 loss: 2.277\n",
      "10000 loss: 2.246\n",
      "12000 loss: 2.180\n",
      "epoch #4\n",
      " 2000 loss: 2.194\n",
      " 4000 loss: 2.151\n",
      " 6000 loss: 2.091\n",
      " 8000 loss: 2.000\n",
      "10000 loss: 2.007\n",
      "12000 loss: 1.942\n",
      "epoch #5\n",
      " 2000 loss: 1.897\n",
      " 4000 loss: 1.800\n",
      " 6000 loss: 1.710\n",
      " 8000 loss: 1.658\n",
      "10000 loss: 1.609\n",
      "12000 loss: 1.554\n",
      "epoch #1\n",
      " 2000 loss: 3.486\n",
      " 4000 loss: 3.355\n",
      " 6000 loss: 3.241\n",
      " 8000 loss: 3.194\n",
      "10000 loss: 3.125\n",
      "12000 loss: 3.092\n",
      "epoch #2\n",
      " 2000 loss: 2.850\n",
      " 4000 loss: 2.843\n",
      " 6000 loss: 2.785\n",
      " 8000 loss: 2.756\n",
      "10000 loss: 2.714\n",
      "12000 loss: 2.687\n",
      "epoch #3\n",
      " 2000 loss: 2.934\n",
      " 4000 loss: 2.903\n",
      " 6000 loss: 2.880\n",
      " 8000 loss: 2.883\n",
      "10000 loss: 2.831\n",
      "12000 loss: 2.853\n",
      "epoch #4\n",
      " 2000 loss: 2.836\n",
      " 4000 loss: 2.829\n",
      " 6000 loss: 2.828\n",
      " 8000 loss: 2.819\n",
      "10000 loss: 2.783\n",
      "12000 loss: 2.768\n",
      "epoch #5\n",
      " 2000 loss: 2.744\n",
      " 4000 loss: 2.716\n",
      " 6000 loss: 2.726\n",
      " 8000 loss: 2.708\n",
      "10000 loss: 2.658\n",
      "12000 loss: 2.660\n"
     ]
    }
   ],
   "source": [
    "print(\"AlexNet with real-time curriculum:\")\n",
    "real_time(trainset, real_time_alex, loss_fn, opt_real_alex, np.arange(-2.0, 2, 1))\n",
    "print(\"\\n\\nDileNet with real-time curriculum:\")\n",
    "real_time(trainset, real_time_dile, loss_fn, opt_real_dile, np.arange(-2.0, 2, 1))\n",
    "print(\"\\n\\nLeNet with real-time curriculum:\")\n",
    "real_time(trainset, real_time_lenet, loss_fn, opt_real_lenet, np.arange(-2.0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we train the same models using no curriculum strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 3.525\n",
      "[1,  4000] loss: 3.466\n",
      "[1,  6000] loss: 3.333\n",
      "[1,  8000] loss: 3.287\n",
      "[1, 10000] loss: 3.244\n",
      "[1, 12000] loss: 3.189\n",
      "[2,  2000] loss: 3.125\n",
      "[2,  4000] loss: 3.089\n",
      "[2,  6000] loss: 3.042\n",
      "[2,  8000] loss: 3.048\n",
      "[2, 10000] loss: 2.994\n",
      "[2, 12000] loss: 2.989\n",
      "[3,  2000] loss: 2.931\n",
      "[3,  4000] loss: 2.915\n",
      "[3,  6000] loss: 2.890\n",
      "[3,  8000] loss: 2.891\n",
      "[3, 10000] loss: 2.861\n",
      "[3, 12000] loss: 2.842\n",
      "[4,  2000] loss: 2.780\n",
      "[4,  4000] loss: 2.765\n",
      "[4,  6000] loss: 2.759\n",
      "[4,  8000] loss: 2.770\n",
      "[4, 10000] loss: 2.759\n",
      "[4, 12000] loss: 2.738\n",
      "[5,  2000] loss: 2.650\n",
      "[5,  4000] loss: 2.635\n",
      "[5,  6000] loss: 2.645\n",
      "[5,  8000] loss: 2.634\n",
      "[5, 10000] loss: 2.649\n",
      "[5, 12000] loss: 2.639\n",
      "Finished Training\n",
      "[1,  2000] loss: 3.338\n",
      "[1,  4000] loss: 3.141\n",
      "[1,  6000] loss: 3.006\n",
      "[1,  8000] loss: 2.975\n",
      "[1, 10000] loss: 2.877\n",
      "[1, 12000] loss: 2.854\n",
      "[2,  2000] loss: 2.716\n",
      "[2,  4000] loss: 2.713\n",
      "[2,  6000] loss: 2.681\n",
      "[2,  8000] loss: 2.658\n",
      "[2, 10000] loss: 2.641\n",
      "[2, 12000] loss: 2.614\n",
      "[3,  2000] loss: 2.460\n",
      "[3,  4000] loss: 2.452\n",
      "[3,  6000] loss: 2.448\n",
      "[3,  8000] loss: 2.448\n",
      "[3, 10000] loss: 2.432\n",
      "[3, 12000] loss: 2.443\n",
      "[4,  2000] loss: 2.186\n",
      "[4,  4000] loss: 2.210\n",
      "[4,  6000] loss: 2.244\n",
      "[4,  8000] loss: 2.271\n",
      "[4, 10000] loss: 2.308\n",
      "[4, 12000] loss: 2.295\n",
      "[5,  2000] loss: 1.924\n",
      "[5,  4000] loss: 2.008\n",
      "[5,  6000] loss: 2.030\n",
      "[5,  8000] loss: 2.098\n",
      "[5, 10000] loss: 2.134\n",
      "[5, 12000] loss: 2.137\n",
      "Finished Training\n",
      "[1,  2000] loss: 3.491\n",
      "[1,  4000] loss: 3.329\n",
      "[1,  6000] loss: 3.263\n",
      "[1,  8000] loss: 3.196\n",
      "[1, 10000] loss: 3.133\n",
      "[1, 12000] loss: 3.104\n",
      "[2,  2000] loss: 3.060\n",
      "[2,  4000] loss: 3.002\n",
      "[2,  6000] loss: 3.001\n",
      "[2,  8000] loss: 2.994\n",
      "[2, 10000] loss: 2.979\n",
      "[2, 12000] loss: 2.960\n",
      "[3,  2000] loss: 2.915\n",
      "[3,  4000] loss: 2.903\n",
      "[3,  6000] loss: 2.906\n",
      "[3,  8000] loss: 2.898\n",
      "[3, 10000] loss: 2.889\n",
      "[3, 12000] loss: 2.889\n",
      "[4,  2000] loss: 2.815\n",
      "[4,  4000] loss: 2.832\n",
      "[4,  6000] loss: 2.841\n",
      "[4,  8000] loss: 2.869\n",
      "[4, 10000] loss: 2.824\n",
      "[4, 12000] loss: 2.841\n",
      "[5,  2000] loss: 2.782\n",
      "[5,  4000] loss: 2.788\n",
      "[5,  6000] loss: 2.800\n",
      "[5,  8000] loss: 2.780\n",
      "[5, 10000] loss: 2.791\n",
      "[5, 12000] loss: 2.803\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "no_curriculum(no_curriculum_alex, loss_fn, opt_alex, 5)\n",
    "no_curriculum(no_curriculum_dile, loss_fn, opt_dile, 5)\n",
    "no_curriculum(no_curriculum_lenet, loss_fn, opt_lenet, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['real_time_alex', 'no_curriculum_alex', 'real_time_dile', \n",
    "               'no_curriculum_dile', 'real_time_lenet', 'no_curriculum_lenet']\n",
    "models_trained = [real_time_alex, no_curriculum_alex, real_time_dile, no_curriculum_dile, real_time_lenet, no_curriculum_lenet]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "We now output the results of both curriculum and non-curriculum based training strategies. As you can see, with limited training there are noticeable improvements to testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_time_alex Accuracy of the network on the 3744 test images: 23 %\n",
      "no_curriculum_alex Accuracy of the network on the 3744 test images: 23 %\n",
      "real_time_dile Accuracy of the network on the 3744 test images: 25 %\n",
      "no_curriculum_dile Accuracy of the network on the 3744 test images: 29 %\n",
      "real_time_lenet Accuracy of the network on the 3744 test images: 18 %\n",
      "no_curriculum_lenet Accuracy of the network on the 3744 test images: 20 %\n"
     ]
    }
   ],
   "source": [
    "for name, model_trained in zip(model_names, models_trained):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model_trained(Variable(images).cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.cuda()).sum()\n",
    "\n",
    "    print(name + ' Accuracy of the network on the 3744 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_time_alex Accuracy of airplane : 41 %\n",
      "real_time_alex Accuracy of backpack : 10 %\n",
      "real_time_alex Accuracy of banana : 46 %\n",
      "real_time_alex Accuracy of bench :  9 %\n",
      "real_time_alex Accuracy of bicycle : 21 %\n",
      "real_time_alex Accuracy of  bird :  7 %\n",
      "real_time_alex Accuracy of  boat : 26 %\n",
      "real_time_alex Accuracy of  book : 20 %\n",
      "real_time_alex Accuracy of bottle :  9 %\n",
      "real_time_alex Accuracy of  bowl :  1 %\n",
      "real_time_alex Accuracy of   car : 11 %\n",
      "real_time_alex Accuracy of carrot : 56 %\n",
      "real_time_alex Accuracy of chair : 25 %\n",
      "real_time_alex Accuracy of clock : 49 %\n",
      "real_time_alex Accuracy of   cow : 25 %\n",
      "real_time_alex Accuracy of   cup :  1 %\n",
      "real_time_alex Accuracy of donut : 29 %\n",
      "real_time_alex Accuracy of  fork : 24 %\n",
      "real_time_alex Accuracy of handbag : 13 %\n",
      "real_time_alex Accuracy of horse : 22 %\n",
      "real_time_alex Accuracy of  kite : 47 %\n",
      "real_time_alex Accuracy of knife : 18 %\n",
      "real_time_alex Accuracy of person : 11 %\n",
      "real_time_alex Accuracy of pottedplant :  3 %\n",
      "real_time_alex Accuracy of sheep : 36 %\n",
      "real_time_alex Accuracy of  sink : 41 %\n",
      "real_time_alex Accuracy of skateboard : 36 %\n",
      "real_time_alex Accuracy of spoon :  1 %\n",
      "real_time_alex Accuracy of surfboard : 23 %\n",
      "real_time_alex Accuracy of tennisracket : 36 %\n",
      "real_time_alex Accuracy of trafficlight : 34 %\n",
      "real_time_alex Accuracy of truck : 12 %\n",
      "real_time_alex Accuracy of umbrella : 28 %\n",
      "real_time_alex Accuracy of  vase : 16 %\n",
      "no_curriculum_alex Accuracy of airplane : 55 %\n",
      "no_curriculum_alex Accuracy of backpack : 40 %\n",
      "no_curriculum_alex Accuracy of banana : 57 %\n",
      "no_curriculum_alex Accuracy of bench : 14 %\n",
      "no_curriculum_alex Accuracy of bicycle : 14 %\n",
      "no_curriculum_alex Accuracy of  bird :  3 %\n",
      "no_curriculum_alex Accuracy of  boat : 30 %\n",
      "no_curriculum_alex Accuracy of  book : 14 %\n",
      "no_curriculum_alex Accuracy of bottle :  2 %\n",
      "no_curriculum_alex Accuracy of  bowl :  0 %\n",
      "no_curriculum_alex Accuracy of   car : 18 %\n",
      "no_curriculum_alex Accuracy of carrot : 50 %\n",
      "no_curriculum_alex Accuracy of chair :  6 %\n",
      "no_curriculum_alex Accuracy of clock : 47 %\n",
      "no_curriculum_alex Accuracy of   cow : 14 %\n",
      "no_curriculum_alex Accuracy of   cup :  5 %\n",
      "no_curriculum_alex Accuracy of donut : 21 %\n",
      "no_curriculum_alex Accuracy of  fork : 23 %\n",
      "no_curriculum_alex Accuracy of handbag :  6 %\n",
      "no_curriculum_alex Accuracy of horse : 36 %\n",
      "no_curriculum_alex Accuracy of  kite : 42 %\n",
      "no_curriculum_alex Accuracy of knife : 16 %\n",
      "no_curriculum_alex Accuracy of person :  1 %\n",
      "no_curriculum_alex Accuracy of pottedplant : 19 %\n",
      "no_curriculum_alex Accuracy of sheep : 53 %\n",
      "no_curriculum_alex Accuracy of  sink : 43 %\n",
      "no_curriculum_alex Accuracy of skateboard : 22 %\n",
      "no_curriculum_alex Accuracy of spoon :  6 %\n",
      "no_curriculum_alex Accuracy of surfboard : 13 %\n",
      "no_curriculum_alex Accuracy of tennisracket : 32 %\n",
      "no_curriculum_alex Accuracy of trafficlight : 15 %\n",
      "no_curriculum_alex Accuracy of truck : 22 %\n",
      "no_curriculum_alex Accuracy of umbrella : 20 %\n",
      "no_curriculum_alex Accuracy of  vase : 16 %\n",
      "real_time_dile Accuracy of airplane : 40 %\n",
      "real_time_dile Accuracy of backpack : 20 %\n",
      "real_time_dile Accuracy of banana : 67 %\n",
      "real_time_dile Accuracy of bench :  6 %\n",
      "real_time_dile Accuracy of bicycle : 18 %\n",
      "real_time_dile Accuracy of  bird : 15 %\n",
      "real_time_dile Accuracy of  boat : 23 %\n",
      "real_time_dile Accuracy of  book : 27 %\n",
      "real_time_dile Accuracy of bottle :  9 %\n",
      "real_time_dile Accuracy of  bowl :  3 %\n",
      "real_time_dile Accuracy of   car : 11 %\n",
      "real_time_dile Accuracy of carrot : 51 %\n",
      "real_time_dile Accuracy of chair : 11 %\n",
      "real_time_dile Accuracy of clock : 52 %\n",
      "real_time_dile Accuracy of   cow : 26 %\n",
      "real_time_dile Accuracy of   cup : 16 %\n",
      "real_time_dile Accuracy of donut :  9 %\n",
      "real_time_dile Accuracy of  fork : 25 %\n",
      "real_time_dile Accuracy of handbag : 10 %\n",
      "real_time_dile Accuracy of horse : 32 %\n",
      "real_time_dile Accuracy of  kite : 53 %\n",
      "real_time_dile Accuracy of knife : 13 %\n",
      "real_time_dile Accuracy of person : 11 %\n",
      "real_time_dile Accuracy of pottedplant : 16 %\n",
      "real_time_dile Accuracy of sheep : 31 %\n",
      "real_time_dile Accuracy of  sink : 43 %\n",
      "real_time_dile Accuracy of skateboard : 34 %\n",
      "real_time_dile Accuracy of spoon :  7 %\n",
      "real_time_dile Accuracy of surfboard : 40 %\n",
      "real_time_dile Accuracy of tennisracket : 30 %\n",
      "real_time_dile Accuracy of trafficlight : 33 %\n",
      "real_time_dile Accuracy of truck : 28 %\n",
      "real_time_dile Accuracy of umbrella : 19 %\n",
      "real_time_dile Accuracy of  vase : 11 %\n",
      "no_curriculum_dile Accuracy of airplane : 53 %\n",
      "no_curriculum_dile Accuracy of backpack : 21 %\n",
      "no_curriculum_dile Accuracy of banana : 58 %\n",
      "no_curriculum_dile Accuracy of bench :  8 %\n",
      "no_curriculum_dile Accuracy of bicycle : 35 %\n",
      "no_curriculum_dile Accuracy of  bird : 30 %\n",
      "no_curriculum_dile Accuracy of  boat : 20 %\n",
      "no_curriculum_dile Accuracy of  book : 17 %\n",
      "no_curriculum_dile Accuracy of bottle :  5 %\n",
      "no_curriculum_dile Accuracy of  bowl :  5 %\n",
      "no_curriculum_dile Accuracy of   car : 16 %\n",
      "no_curriculum_dile Accuracy of carrot : 68 %\n",
      "no_curriculum_dile Accuracy of chair :  3 %\n",
      "no_curriculum_dile Accuracy of clock : 45 %\n",
      "no_curriculum_dile Accuracy of   cow : 33 %\n",
      "no_curriculum_dile Accuracy of   cup : 10 %\n",
      "no_curriculum_dile Accuracy of donut : 34 %\n",
      "no_curriculum_dile Accuracy of  fork : 26 %\n",
      "no_curriculum_dile Accuracy of handbag : 15 %\n",
      "no_curriculum_dile Accuracy of horse : 20 %\n",
      "no_curriculum_dile Accuracy of  kite : 63 %\n",
      "no_curriculum_dile Accuracy of knife : 24 %\n",
      "no_curriculum_dile Accuracy of person :  8 %\n",
      "no_curriculum_dile Accuracy of pottedplant : 13 %\n",
      "no_curriculum_dile Accuracy of sheep : 57 %\n",
      "no_curriculum_dile Accuracy of  sink : 38 %\n",
      "no_curriculum_dile Accuracy of skateboard : 44 %\n",
      "no_curriculum_dile Accuracy of spoon : 10 %\n",
      "no_curriculum_dile Accuracy of surfboard : 45 %\n",
      "no_curriculum_dile Accuracy of tennisracket : 46 %\n",
      "no_curriculum_dile Accuracy of trafficlight : 41 %\n",
      "no_curriculum_dile Accuracy of truck : 31 %\n",
      "no_curriculum_dile Accuracy of umbrella : 15 %\n",
      "no_curriculum_dile Accuracy of  vase : 17 %\n",
      "real_time_lenet Accuracy of airplane : 17 %\n",
      "real_time_lenet Accuracy of backpack :  8 %\n",
      "real_time_lenet Accuracy of banana : 43 %\n",
      "real_time_lenet Accuracy of bench :  8 %\n",
      "real_time_lenet Accuracy of bicycle : 18 %\n",
      "real_time_lenet Accuracy of  bird :  6 %\n",
      "real_time_lenet Accuracy of  boat : 20 %\n",
      "real_time_lenet Accuracy of  book :  9 %\n",
      "real_time_lenet Accuracy of bottle :  5 %\n",
      "real_time_lenet Accuracy of  bowl :  5 %\n",
      "real_time_lenet Accuracy of   car : 10 %\n",
      "real_time_lenet Accuracy of carrot : 58 %\n",
      "real_time_lenet Accuracy of chair :  5 %\n",
      "real_time_lenet Accuracy of clock : 37 %\n",
      "real_time_lenet Accuracy of   cow : 16 %\n",
      "real_time_lenet Accuracy of   cup : 20 %\n",
      "real_time_lenet Accuracy of donut : 18 %\n",
      "real_time_lenet Accuracy of  fork :  9 %\n",
      "real_time_lenet Accuracy of handbag : 28 %\n",
      "real_time_lenet Accuracy of horse :  3 %\n",
      "real_time_lenet Accuracy of  kite : 25 %\n",
      "real_time_lenet Accuracy of knife :  6 %\n",
      "real_time_lenet Accuracy of person :  6 %\n",
      "real_time_lenet Accuracy of pottedplant :  9 %\n",
      "real_time_lenet Accuracy of sheep : 38 %\n",
      "real_time_lenet Accuracy of  sink : 34 %\n",
      "real_time_lenet Accuracy of skateboard : 40 %\n",
      "real_time_lenet Accuracy of spoon :  5 %\n",
      "real_time_lenet Accuracy of surfboard : 14 %\n",
      "real_time_lenet Accuracy of tennisracket : 19 %\n",
      "real_time_lenet Accuracy of trafficlight : 31 %\n",
      "real_time_lenet Accuracy of truck : 16 %\n",
      "real_time_lenet Accuracy of umbrella : 20 %\n",
      "real_time_lenet Accuracy of  vase : 10 %\n",
      "no_curriculum_lenet Accuracy of airplane : 56 %\n",
      "no_curriculum_lenet Accuracy of backpack :  8 %\n",
      "no_curriculum_lenet Accuracy of banana : 57 %\n",
      "no_curriculum_lenet Accuracy of bench :  8 %\n",
      "no_curriculum_lenet Accuracy of bicycle : 26 %\n",
      "no_curriculum_lenet Accuracy of  bird :  6 %\n",
      "no_curriculum_lenet Accuracy of  boat : 11 %\n",
      "no_curriculum_lenet Accuracy of  book : 14 %\n",
      "no_curriculum_lenet Accuracy of bottle :  1 %\n",
      "no_curriculum_lenet Accuracy of  bowl :  1 %\n",
      "no_curriculum_lenet Accuracy of   car : 25 %\n",
      "no_curriculum_lenet Accuracy of carrot : 58 %\n",
      "no_curriculum_lenet Accuracy of chair :  1 %\n",
      "no_curriculum_lenet Accuracy of clock : 40 %\n",
      "no_curriculum_lenet Accuracy of   cow : 32 %\n",
      "no_curriculum_lenet Accuracy of   cup :  0 %\n",
      "no_curriculum_lenet Accuracy of donut : 28 %\n",
      "no_curriculum_lenet Accuracy of  fork :  7 %\n",
      "no_curriculum_lenet Accuracy of handbag :  4 %\n",
      "no_curriculum_lenet Accuracy of horse :  6 %\n",
      "no_curriculum_lenet Accuracy of  kite : 32 %\n",
      "no_curriculum_lenet Accuracy of knife : 13 %\n",
      "no_curriculum_lenet Accuracy of person :  1 %\n",
      "no_curriculum_lenet Accuracy of pottedplant : 14 %\n",
      "no_curriculum_lenet Accuracy of sheep : 37 %\n",
      "no_curriculum_lenet Accuracy of  sink : 38 %\n",
      "no_curriculum_lenet Accuracy of skateboard : 33 %\n",
      "no_curriculum_lenet Accuracy of spoon :  5 %\n",
      "no_curriculum_lenet Accuracy of surfboard : 22 %\n",
      "no_curriculum_lenet Accuracy of tennisracket : 21 %\n",
      "no_curriculum_lenet Accuracy of trafficlight : 35 %\n",
      "no_curriculum_lenet Accuracy of truck : 10 %\n",
      "no_curriculum_lenet Accuracy of umbrella : 16 %\n",
      "no_curriculum_lenet Accuracy of  vase : 24 %\n"
     ]
    }
   ],
   "source": [
    "for name, model_trained in zip(model_names, models_trained):\n",
    "    class_correct = list(0. for i in range(34))\n",
    "    class_total = list(0. for i in range(34))\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model_trained(Variable(images).cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels.cuda()).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i]\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(34):\n",
    "        print(name + ' Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
